\chapter{Introduzione}

\section{Che Cos'è l'Apprendimento Automatico?}

\dfn{Machine Learning}{
	Un programma informatico apprende dall'esperienza $E$ rispetto a una classe di task $T$ e una performance $P$, se la sua performance nel task $T$, misurata da $P$, aumenta con l'esperienza $E$.
}

\paragraph{Sostanzialmente:}

\begin{itemize}
	\item L'esperienza viene data sotto forma di esempi "risolti" al computer.
	\item Un task (compito) da risolvere.
	\item Con un modo per valutare la risoluzione (performance).
\end{itemize}

\subsection{Terminologia}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.45]{01/terminology.png}
	\caption{Terminologia.}
\end{figure}

\begin{itemize}
	\item Attributi (Features): le colonne.
	\item Etichetta (Classe): elemento che indica come risolvere un task.
	\item Istanza (Sample): una riga.
	\item Valori: le celle.
	\item Set di Training: insieme su cui si va a dedurre una regola per classificare.
	\item Test Set: insieme per vedere quanto si sarà accurati su insiemi futuri.
\end{itemize}

\nt{Si usa un set diverso per il training e il test perché se si usasse lo stesso il modello farebbe risultati elevati essendo addestrato su quello.}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{01/terminology2.png}
	\caption{Terminologia 2.}
\end{figure}

\paragraph{Immaginando gli oggetti in un qualche campo euclideo:}

\begin{itemize}
	\item \fancyglitter{Features vector:} ogni esempio corrisponde a un vettore.
	\item \fancyglitter{Attribute space:} l'insieme di tutti gli esempi.
\end{itemize}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{01/table.png}
	\caption{Tabella di riferimento.}
\end{figure}

\dfn{Learning (Training)}{
	Il learning è un processo in cui si usano algoritmi di apprendimento automatico per costruire dei modelli.
	\begin{itemize}
		\item I dati utilizzati in questo processo sono detti training data.
		\item Ogni istanza è un training example.
		\item L'insieme di tutti i training example è il training set.
	\end{itemize}
}

\nt{Un modello addestrato corrisponde a una serie di regole sui dati, quindi si chiama anche \fancyglitter{ipotesi} e le regole sono i \fancyglitter{fatti} (grounded-truth).}

\subsection{Tasks}

\dfn{Tasks predittivi}{
	Un task predittivo è focalizzato sul prevedere una variabile sulla base degli esempi. Si parte da problemi vecchi per trovare la soluzione a nuovi problemi.
}

\paragraph{I tasks predittivi possono essere:}

\begin{itemize}
	\item \fancyglitter{Binari e Multi-classe:} di categorizzazione.
	\item \fancyglitter{Regressivi:} con un target numerico.
	\item \fancyglitter{Clustering:} un target sconosciuto.
\end{itemize}

\dfn{Tasks descrittivi}{
	Un task descrittivo si concentra sul fornire regolarità nel dataset.
}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{01/tasks.png}
	\caption{Vari tasks.}
\end{figure}

\paragraph{Assunzione:}

\begin{itemize}
	\item Si assume che i dati siano \fancyglitter{indipendenti}.
	\item Si assume che i dati siano \fancyglitter{identicamente distribuiti}.
\end{itemize}
\subsection{Spazio di Ipotesi}

Nei sistemi \fancyglitter{assiomatici} il processo di derivare un teorema da assiomi è detto \fancyglitter{deduzione}. Questo è un processo corretto se si assume che gli assiomi siano veri. L'\fancyglitter{induzione} è il processo opposto ed è il principio su cui si basa tutto l'apprendimento automatico.


\nt{ATTENZIONE: l'induzione come intesa in questo corso non è l'induzione matematica.}

\clm{Deduzione vs. Induzione}{}{
	\begin{itemize}
		\item La deduzione è valida: assumere le premesse vere garantisce che le conclusioni siano vere.
		\item L'induzione non è valida come forma di ragionamento: non garantisce che  la conclusione sia vera anche se tutte le osservazioni sono corrette.
	\end{itemize}
}

\dfn{Boolean Concept Learning}{
	L'obiettivo è quello di apprendere una funzione booleana $h: \mcX \mapsto \{0,1\}$
}

\nt{Siamo nel campo del \fancyglitter{symbolic concept learning}, studiato nel campo dellla \fancyglitter{inductive logic programming}.}

\dfn{Spazio di Ipotesi}{
	Lo spazio di Ipotesi è l'insieme di tutte le possibili ipotesi che possono essere imparate da un algoritmo di apprendimento.
}

\nt{Il Machine Learning è la ricerca attraverso lo spazio delle ipotesi per trovare l'insieme di tutte le ipotesi che sono consistenti con i training data e selezionare i migliori secondo un qualche criterio.}

\cor{Spazio delle Versioni}{
	Sottoinsiem dello spazio delle ipotesi in cui tutte le ipotesi sono consistenti con il training data.
}

\dfn{Bias Induttivo}{
	Il bias induttivo è un insieme di assunzioni che permette agli algoritmi di apprendimento di scegliere un'ipotesi dallo spazio delle versioni.
}

\clm{}{}{
	\begin{itemize}
		\item Ogni algoritmo ha un bias induttivo.
		\item L'unica altra possibilità sarebbe quella di scegliere a caso, ma è dumb as fuck come cosa.
	\end{itemize}
}

\qs{}{Possiamo adottare un'algoritmo con il miglior bias induttivo possibile?}

\paragraph{\fancyglitter{Rasoio di Occam:}} tra ipotesi in competizione si sceglie la più semplice.

\begin{itemize}
	\item Sarebbe troppo bello se fosse così.
	\item Ma cosa vuol dire "più semplice"?
	      \begin{itemize}
		      \item Minor numero di parole/termini?
		      \item Più facile da interpretare?
		      \item Che faccia meno assunzioni sui dati?
	      \end{itemize}
\end{itemize}

\subsection{No Free Lunch Theorem}

\thm{No Free Lunch Theorem}{
	Nessun algoritmo è universalmente migliore di tutti gli altri quando la loro performance è la media su tutti i possibili problemi.
}


\paragraph{Denotiamo con:}

\begin{itemize}
	\item $P(h|X,\mathcal{L}_a)$: la probabilità che l'algoritmo $\mathcal{L}_a$ restituisca l'ipotesi $h$ dato il training set $X$.
	\item $f$ la funzione che si vuole apprendere.
	\item L'errore out-of-sample, media dell'errore sugli esempi non nel training set:
	      \[
		      E_{ote}(\mathcal{L}_a \mid X, f)
		      = \sum_{h} \sum_{x \in \mathcal{X}-X} P(x)\,\mathbb{I}(h(x)\ne f(x))\,P(h \mid X, \mathcal{L}_a)
	      \]
\end{itemize}

\paragraph{Sommando tutte le possibili funzioni $f_i$ otteniamo:}

\[
	\sum_{f} E_{ote}(\mathcal{L}_a \mid X, f)
	= \sum_f \sum_h \sum_{x \in \mathcal{X}-X} P(x)\mathbb{I}(h(x)\neq f(x))P(h \mid X, \mathcal{L}_a)
\]

\[
	= \sum_{x \in \mathcal{X}-X} P(x)\sum_h P(h \mid X,\mathcal{L}_a)\sum_f \mathbb{I}(h(x)\neq f(x))
\]

\[
	= \sum_{x \in \mathcal{X}-X} P(x)\sum_h P(h \mid X,\mathcal{L}_a)\,\frac{1}{2} 2^{|\mathcal{X}|}
\]

\[
	= \frac{1}{2} 2^{|\mathcal{X}|} \sum_{x \in \mathcal{X}-X} P(x)\sum_h P(h \mid X,\mathcal{L}_a)
\]

\[
	= \frac{1}{2} 2^{|\mathcal{X}|} \sum_{x \in \mathcal{X}-X} P(x)\cdot 1
\]

\nt{
	Il No Free Lunch theorem si basa sull'assunzione che tutte le funzioni $f$ sono ugualmente probabili. Ciò implica che la scelta dell'algoritmo di learning dovrebbe essere guidata dalle caratteristiche del problema.
}

\section{Selezione del Modello e Valutazione}

\subsection{Errore Empirico e Overfitting}

\qs{}{Come misuriamo la bontà di un modello?}

\begin{itemize}
	\item \fancyglitter{Error Rate:} si contano le predizioni sbagliate fatte, $E = \frac{a}{m}$.
	\item \fancyglitter{Accuratezza:} accuracy = $ 1 - E$.
	\item L'errore valutato sul training set è chiamato \fancyglitter{empirical error} o \fancyglitter{training error}.
	\item L'errore valutato su nuovi esempi al di fuori del training set è chiamato \fancyglitter{generalization error} o \fancyglitter{test error}.
\end{itemize}

\nt{
	Lo scopo dell'apprendimento è quello di minimizzare il generalization error. Però non si può fare direttamente, per cui si tende a minimizzare l'errore di training.
}

\dfn{Overfitting}{
	Quando un modello si adatti ai dati troppo bene si rischia che esso sia troppo legato ai dati del training.
}

\nt{Il fenomeno opposto è l'underfitting.}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.45]{01/esempio.png}
	\caption{Overfitting e Underfitting.}
\end{figure}

\clm{}{}{
	\begin{itemize}
		\item L'underfitting si risolve facilmente usando modelli più complessi.
		\item L'overfitting richiede un bilancio tra complessità del modello e capacità di generalizzazione.
		\item Molti algoritmi di apprendimento automatico hanno meccanismi per prevenire l'overfitting come \fancyglitter{tecniche di regolarizzazione} o \fancyglitter{early stopping}.
	\end{itemize}
}

\subsection{Metodi di Valutazione}

Quello che si vuole fare è misurare l'errore di generalizzazione su nuovi esempi.


\dfn{Hold-Out Validation}{Dato l'insieme dei dati se ne tiene nascosta una parte all'algoritmo (test set).}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.4]{01/hold.png}
	\caption{Validazione Hold-Out.}
\end{figure}

\paragraph{Gli split:}

\begin{itemize}
	\item Split semplice: si prendono i primi $n$ dati come test set. I problemi sorgono quando i dati non sono ben distribuiti.
	\item Split stratificato: si mescolano i dati prima di estrarli.
\end{itemize}

\clm{}{}{
	\begin{itemize}
		\item Ripetere la validazione Hold-Out più volte produrra risultati diversi poiché lo split è randomico.
		\item Per ottenere una stima migliore dell'errore di generalizzazione possiamo ripetere più volte e fare una media.
		\item La media di $n$ variabili indipendenti  è una nuova variabile avente media:
		      \[
			      \displaystyle\frac{1}{n}\displaystyle\sum_{i = 1}^{n} E_i = \displaystyle\frac{1}{n}\displaystyle\sum_{i = 1}^{n} \mu = \mu
		      \]
		\item E varianza:
		      \[
			      \displaystyle\frac{1}{n^2}\displaystyle\sum_{i = 1}^{n} \sigma^2 = \displaystyle\frac{\sigma^2}{n}
		      \]
	\end{itemize}
}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.55]{01/out.png}
	\caption{Validazione Hold-Out con differenti split ratio.}
\end{figure}

\nt{Se il dataset è abbastanza grande la validazione Hold-Out è semplice ed efficacie. Se il dataset è piccolo si ricorre alla \fancyglitter{Cross-Validation}.}

\dfn{Cross-Validation}{
	Ripetizione dell'Hold-Out validation $k$ volte, la prima volta si usa come test set $[0, \frac{1}{k}]$, la seconda $[\frac{1}{k}, \frac{2}{k}]$ fino all'ultima in cui si usa $[\frac{k-1}{k}, 1]$.
}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.55]{01/cross.png}
	\caption{Cross-Validation.}
\end{figure}

\qs{}{
	Perché usare la Cross-Validation invece di prendere la media degli Hold-Out ripetuti?
}

\begin{itemize}
	\item Nella Cross-Validation ogni esempio è usato come test esattamente una volta, mentre ripetendo Hold-Out alcuni esempi possono essere usati più volte e alcuni mai usati.
	\item È più accurata, ma se il dataset è sufficientemente grande la differenza è trascurabile.
\end{itemize}

\dfn{Leave-One-Out}{
	Ogni esempio è usato per testare esattamente una volra e tutti gli altri esempi sono usati per il training. L'errore di generalizzazione ha varianza molto bassa.
}

\nt{
	È lo stimatore più preciso, ma è estremamente costoso dal punto di vista computazionale.
}

\dfn{Bootstrapping}{
	Dato un dataset $D$ con $m$ esempi l'obiettivo è di stimare l'errore con l'intero dataset.  L'idea è quella di creare multipli esempi di bootstrap dal dataset originale per rimpiazzare gli esempi che verranno usati per stimare l'errore.
}

\nt{Gli esempi di bootstrap non sono disgiunti: diversi esempi saranno ripetuti e alcuni non compariranno.}

\paragraph{Probabilità che un esempio non sarà selezionato:}

$P(sns) = (1 - \displaystyle\frac{1}{m})^m$

\clm{}{}{
	\begin{itemize}
		\item Consideriamo $D'$ come bootstrap di $D$.
		\item Possiamo addestrare un modello su $D'$ e valutare la performance su $D - D'$.
		\item È particolarmente utile su piccoli dataset o quando non c'è un modo decente di splittare in training e dataset.
	\end{itemize}
}

\begin{itemize}
	\item \fancyglitter{Parametri:} gli oggetti modificati dall'algoritmo di apprendimento per adattarsi ai dati.
	\item \fancyglitter{Hyperparametri:} parametri dell'algoritmo di apprendimento in sè e per sè.
\end{itemize}

\dfn{Parameter Tuning}{
	Scegliere il miglior modello da un insieme di modelli candidati o scegliere i miglior Hyperparametri per un dato algoritmo.
}

\nt{
	Bisogna prestare attenzione a non fare overfitting sul test set.
}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.45]{01/tuning.png}
	\caption{Tuning.}
\end{figure}


\subsection{Misure di Performance}

Misure di performance differenti riflettono le varie domande di task e producono diversi risultati. Scegliere la misura giusta è cruciale per una valutazione equa del modello.

\dfn{Mean Square Error (MSE)}{
È una misura per task di regressione. Se si conosce la distribuzione dei dati di $D$:

\[
E(f; \mathcal{D})
= \mathbb{E}_{(x,y)\sim \mathcal{D}} \big[ (f(x) - y)^2 \big]
= \int_{(x,y)\sim \mathcal{D}} (f(x) - y)^2 p(x,y)\, dx\, dy
\]
}

\paragraph{Però spesso non si conosce la distribuzione, per cui si stima MSE come:}

\[
	E(f; D) = \frac{1}{m} \sum_{i=1}^m (f(x_i) - y_i)^2.
\]

\dfn{Error Rate}{
Assumendo nota la conoscenza della distribuzione $D$:
\[
E(f; D) = \mathbb{E}_{(\mathbf{x},y)\sim D} [\mathbb{I}(f(\mathbf{x}) \neq y)] = \int_{(\mathbf{x},y)\sim D} \mathbb{I}(f(\mathbf{x}) \neq y)p(\mathbf{x}, y)dx dy
\]
dove $\bbI$ è la funzione indicatore che restituisce 1 se la condizione è vera e 0 altrimenti.
}

\paragraph{Si può stimare come:}
\[
	E(f; D) = \frac{1}{m} \sum_{i=1}^{m} \mathbb{I}(f(\mathbf{x}_i) \neq y_i)
\]

\nt{L'accuratezza si calcola come complemento dell'error rate.}

\dfn{Matrice di Confusione}{
	Si tratta di una matrice 2x2 con esempi e predizioni (su righe o su colonne dipende da come gira all'autore).
}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{01/confusion.png}
	\caption{Matrice di confusione.}
\end{figure}

\paragraph{Altre misure:}

\begin{itemize}
	\item \fancyglitter{Precision:} l'abilità di identificare i casi positivi.
	      \begin{center}
		      Precision = $\frac{TP}{TP + FP}$
	      \end{center}
	\item \fancyglitter{Recall:} l'abilità di evitare di etichettare in modo sbagliato.
	      \begin{center}
		      Recall = $\frac{TP}{TP + FN}$
	      \end{center}
\end{itemize}

\clm{}{}{
	\begin{itemize}
		\item Precision e Recall sono spesso usate insieme perché complementari.
		\item Solitamente l'aumento di una implica la diminuzione dell'altra.
	\end{itemize}
}

\dfn{P-R Plots}{
	Assumendo di avere un classificatore che restituisce una misura di confidenza per ogni campione indicando quanto sia confidente che il campione appartenga alla classe positiva, s può variare un threshold per ottenere diversi valori di precision e recall.
}
\begin{figure}[h]

	\centering
	\includegraphics[scale=0.5]
	{01/pr.png}
	\caption{Esempio di P-R Plot.}
\end{figure}

\qs{}{Come selezioniamo il miglior modello date poche curve P-R?}

\begin{itemize}
	\item Fissare una data precision e prendere il modello con la maggiore recall (e viceversa).
	\item Prendere il miglior \fancyglitter{break-even point}: in cui precision e recall sono uguali.
\end{itemize}

\dfn{F1-Score}{
	Media armonizzata di precision e recall:
	\[
		F1 = \displaystyle\frac{2}{\displaystyle\frac{1}{\text{precision}} + \displaystyle\frac{1}{\text{recall}}}
	\]

}

\nt{Si può generalizzare come F$_\beta$-score in cui si dà un peso diverso a precision e recall.}

\paragraph{Per ottenere la media dei contributi di diverse matrici di confusione ci sono 2 modi:}

\begin{itemize}
	\item \fancyglitter{Macro-averaging:} si computa la precision (P) e la recall (R) per ogni matrice di confusione.
	      \[
		      \text{macro-}P = \frac{1}{n} \sum_{i=1}^{n} P_i
	      \]
	      \[
		      \text{macro-}R = \frac{1}{n} \sum_{i=1}^{n} R_i
	      \]
	      \[
		      \text{macro-}F_1 = \frac{2 \times \text{macro-}P \times \text{macro-}R}{\text{macro-}P + \text{macro-}R}
	      \]

	\item \fancyglitter{Micro-averaging:} si calcola la media degli elementi sulla matrice di confusione.
	      \[
		      \text{micro-}P = \frac{\overline{TP}}{\overline{TP} + \overline{FP}}
	      \]
	      \[
		      \text{micro-}R = \frac{\overline{TP}}{\overline{TP} + \overline{FN}}
	      \]
	      \[
		      \text{micro-}F_1 = \frac{2 \times \text{micro-}P \times \text{micro-}R}{\text{micro-}P + \text{micro-}R}
	      \]
\end{itemize}

\dfn{ROC Plot}{
	Invece di usare precision e recall si fa un plot dei false positive rate (FPR) contro il true positive rate (TPR) per ottenere la Receiver Operating Characteristic curve (ROC):
	\[
		FPR = \frac{FP}{FP + TN} = \frac{FP}{\# \text{ actual negatives}}
	\]
	\[
		TPR = \frac{TP}{TP + FN} = \frac{TP}{\# \text{ actual positives}}
	\]
}

\begin{figure}[h]

	\centering
	\includegraphics[scale=0.5]
	{01/roc.png}
	\caption{ROC Plot.}
\end{figure}

\begin{itemize}
	\item \fancyglitter{ROC Heaven:} classificatore perfetto, non sbaglia mai.
	\item \fancyglitter{ROC Hell:} classificatore che sbaglia sempre.
\end{itemize}

\nt{Avere un classificatore perfetto o uno che sbaglia sempre è la stessa cosa (basta prendere il contrario di cosa restituisce). Il punto peggiore è nel mezzo: tira sempre a caso.}

\paragraph{Area sotto la curva (AUC):}
\[
	1 - \text{AUC} = \ell_{\text{rank}} \triangleq \frac{1}{m^+m^-} \sum_{\mathbf{x}^+ \in D^+} \sum_{\mathbf{x}^- \in D^-} \mathbb{I}[f(\mathbf{x}^+) < f(\mathbf{x}^-)] + \frac{1}{2}\mathbb{I}[f(\mathbf{x}^+) = f(\mathbf{x}^-)]
\]

\dfn{Cost-Sensitive Error Rate}{
	Il cost-sensitive error rate si calcola come:
	\[
		E(f; D; \text{cost}) = \frac{1}{m} \left( \sum_{(\mathbf{x},y) \in D^+} \mathbb{I}[f(\mathbf{x}) \neq y] \times \text{cost}_{01} + \sum_{(\mathbf{x},y) \in D^-} \mathbb{I}[f(\mathbf{x}) \neq y] \times \text{cost}_{10} \right)
	\]
}

\subsection{Comparison Test}


