\chapter{Introduzione}

\section{Che Cos'è l'Apprendimento Automatico?}

\dfn{Machine Learning}{
	Un programma informatico apprende dall'esperienza $E$ rispetto a una classe di task $T$ e una performance $P$, se la sua performance nel task $T$, misurata da $P$, aumenta con l'esperienza $E$.
}

\paragraph{Sostanzialmente:}

\begin{itemize}
	\item L'esperienza viene data sotto forma di esempi "risolti" al computer.
	\item Un task (compito) da risolvere.
	\item Con un modo per valutare la risoluzione (performance).
\end{itemize}

\subsection{Terminologia}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.45]{01/terminology.png}
	\caption{Terminologia.}
\end{figure}

\begin{itemize}
	\item Attributi (Features): le colonne.
	\item Etichetta (Classe): elemento che indica come risolvere un task.
	\item Istanza (Sample): una riga.
	\item Valori: le celle.
	\item Set di Training: insieme su cui si va a dedurre una regola per classificare.
	\item Test Set: insieme per vedere quanto si sarà accurati su insiemi futuri.
\end{itemize}

\nt{Si usa un set diverso per il training e il test perché se si usasse lo stesso il modello farebbe risultati elevati essendo addestrato su quello.}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{01/terminology2.png}
	\caption{Terminologia 2.}
\end{figure}

\paragraph{Immaginando gli oggetti in un qualche campo euclideo:}

\begin{itemize}
	\item \fancyglitter{Features vector:} ogni esempio corrisponde a un vettore.
	\item \fancyglitter{Attribute space:} l'insieme di tutti gli esempi.
\end{itemize}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{01/table.png}
	\caption{Tabella di riferimento.}
\end{figure}

\dfn{Learning (Training)}{
	Il learning è un processo in cui si usano algoritmi di apprendimento automatico per costruire dei modelli.
	\begin{itemize}
		\item I dati utilizzati in questo processo sono detti training data.
		\item Ogni istanza è un training example.
		\item L'insieme di tutti i training example è il training set.
	\end{itemize}
}

\nt{Un modello addestrato corrisponde a una serie di regole sui dati, quindi si chiama anche \fancyglitter{ipotesi} e le regole sono i \fancyglitter{fatti} (grounded-truth).}

\subsection{Tasks}

\dfn{Tasks predittivi}{
	Un task predittivo è focalizzato sul prevedere una variabile sulla base degli esempi. Si parte da problemi vecchi per trovare la soluzione a nuovi problemi.
}

\paragraph{I tasks predittivi possono essere:}

\begin{itemize}
	\item \fancyglitter{Binari e Multi-classe:} di categorizzazione.
	\item \fancyglitter{Regressivi:} con un target numerico.
	\item \fancyglitter{Clustering:} un target sconosciuto.
\end{itemize}

\dfn{Tasks descrittivi}{
	Un task descrittivo si concentra sul fornire regolarità nel dataset.
}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{01/tasks.png}
	\caption{Vari tasks.}
\end{figure}

\paragraph{Assunzione:}

\begin{itemize}
	\item Si assume che i dati siano \fancyglitter{indipendenti}.
	\item Si assume che i dati siano \fancyglitter{identicamente distribuiti}.
\end{itemize}
\subsection{Spazio di Ipotesi}

Nei sistemi \fancyglitter{assiomatici} il processo di derivare un teorema da assiomi è detto \fancyglitter{deduzione}. Questo è un processo corretto se si assume che gli assiomi siano veri. L'\fancyglitter{induzione} è il processo opposto ed è il principio su cui si basa tutto l'apprendimento automatico.


\nt{ATTENZIONE: l'induzione come intesa in questo corso non è l'induzione matematica.}

\clm{Deduzione vs. Induzione}{}{
	\begin{itemize}
		\item La deduzione è valida: assumere le premesse vere garantisce che le conclusioni siano vere.
		\item L'induzione non è valida come forma di ragionamento: non garantisce che  la conclusione sia vera anche se tutte le osservazioni sono corrette.
	\end{itemize}
}

\dfn{Boolean Concept Learning}{
	L'obiettivo è quello di apprendere una funzione booleana $h: \mcX \mapsto \{0,1\}$
}

\nt{Siamo nel campo del \fancyglitter{symbolic concept learning}, studiato nel campo dellla \fancyglitter{inductive logic programming}.}

\dfn{Spazio di Ipotesi}{
	Lo spazio di Ipotesi è l'insieme di tutte le possibili ipotesi che possono essere imparate da un algoritmo di apprendimento.
}

\nt{Il Machine Learning è la ricerca attraverso lo spazio delle ipotesi per trovare l'insieme di tutte le ipotesi che sono consistenti con i training data e selezionare i migliori secondo un qualche criterio.}

\cor{Spazio delle Versioni}{
	Sottoinsiem dello spazio delle ipotesi in cui tutte le ipotesi sono consistenti con il training data.
}

\dfn{Bias Induttivo}{
	Il bias induttivo è un insieme di assunzioni che permette agli algoritmi di apprendimento di scegliere un'ipotesi dallo spazio delle versioni.
}

\clm{}{}{
	\begin{itemize}
		\item Ogni algoritmo ha un bias induttivo.
		\item L'unica altra possibilità sarebbe quella di scegliere a caso, ma è dumb as fuck come cosa.
	\end{itemize}
}

\qs{}{Possiamo adottare un'algoritmo con il miglior bias induttivo possibile?}

\paragraph{\fancyglitter{Rasoio di Occam:}} tra ipotesi in competizione si sceglie la più semplice.

\begin{itemize}
	\item Sarebbe troppo bello se fosse così.
	\item Ma cosa vuol dire "più semplice"?
	      \begin{itemize}
		      \item Minor numero di parole/termini?
		      \item Più facile da interpretare?
		      \item Che faccia meno assunzioni sui dati?
	      \end{itemize}
\end{itemize}

\subsection{No Free Lunch Theorem}

\thm{No Free Lunch Theorem}{
	Nessun algoritmo è universalmente migliore di tutti gli altri quando la loro performance è la media su tutti i possibili problemi.
}


\paragraph{Denotiamo con:}

\begin{itemize}
	\item $P(h|X,\mathcal{L}_a)$: la probabilità che l'algoritmo $\mathcal{L}_a$ restituisca l'ipotesi $h$ dato il training set $X$.
	\item $f$ la funzione che si vuole apprendere.
	\item L'errore out-of-sample, media dell'errore sugli esempi non nel training set:
	      \[
		      E_{ote}(\mathcal{L}_a \mid X, f)
		      = \sum_{h} \sum_{x \in \mathcal{X}-X} P(x)\,\mathbb{I}(h(x)\ne f(x))\,P(h \mid X, \mathcal{L}_a)
	      \]
\end{itemize}

\paragraph{Sommando tutte le possibili funzioni $f_i$ otteniamo:}

\[
	\sum_{f} E_{ote}(\mathcal{L}_a \mid X, f)
	= \sum_f \sum_h \sum_{x \in \mathcal{X}-X} P(x)\mathbb{I}(h(x)\neq f(x))P(h \mid X, \mathcal{L}_a)
\]

\[
	= \sum_{x \in \mathcal{X}-X} P(x)\sum_h P(h \mid X,\mathcal{L}_a)\sum_f \mathbb{I}(h(x)\neq f(x))
\]

\[
	= \sum_{x \in \mathcal{X}-X} P(x)\sum_h P(h \mid X,\mathcal{L}_a)\,\frac{1}{2} 2^{|\mathcal{X}|}
\]

\[
	= \frac{1}{2} 2^{|\mathcal{X}|} \sum_{x \in \mathcal{X}-X} P(x)\sum_h P(h \mid X,\mathcal{L}_a)
\]

\[
	= \frac{1}{2} 2^{|\mathcal{X}|} \sum_{x \in \mathcal{X}-X} P(x)\cdot 1
\]

\nt{
	Il No Free Lunch theorem si basa sull'assunzione che tutte le funzioni $f$ sono ugualmente probabili. Ciò implica che la scelta dell'algoritmo di learning dovrebbe essere guidata dalle caratteristiche del problema.
}

\section{Selezione del Modello e Valutazione}

\subsection{Errore Empirico e Overfitting}

\qs{}{Come misuriamo la bontà di un modello?}

\begin{itemize}
	\item \fancyglitter{Error Rate:} si contano le predizioni sbagliate fatte, $E = \frac{a}{m}$.
	\item \fancyglitter{Accuratezza:} accuracy = $ 1 - E$.
	\item L'errore valutato sul training set è chiamato \fancyglitter{empirical error} o \fancyglitter{training error}.
	\item L'errore valutato su nuovi esempi al di fuori del training set è chiamato \fancyglitter{generalization error} o \fancyglitter{test error}.
\end{itemize}

\nt{
	Lo scopo dell'apprendimento è quello di minimizzare il generalization error. Però non si può fare direttamente, per cui si tende a minimizzare l'errore di training.
}

\dfn{Overfitting}{
	Quando un modello si adatti ai dati troppo bene si rischia che esso sia troppo legato ai dati del training.
}

\nt{Il fenomeno opposto è l'underfitting.}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.45]{01/esempio.png}
	\caption{Overfitting e Underfitting.}
\end{figure}

\clm{}{}{
	\begin{itemize}
		\item L'underfitting si risolve facilmente usando modelli più complessi.
		\item L'overfitting richiede un bilancio tra complessità del modello e capacità di generalizzazione.
		\item Molti algoritmi di apprendimento automatico hanno meccanismi per prevenire l'overfitting come \fancyglitter{tecniche di regolarizzazione} o \fancyglitter{early stopping}.
	\end{itemize}
}

\subsection{Metodi di Valutazione}

Quello che si vuole fare è misurare l'errore di generalizzazione su nuovi esempi.


\dfn{Hold-Out Validation}{Dato l'insieme dei dati se ne tiene nascosta una parte all'algoritmo (test set).}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.4]{01/hold.png}
	\caption{Validazione Hold-Out.}
\end{figure}

\paragraph{Gli split:}

\begin{itemize}
	\item Split semplice: si prendono i primi $n$ dati come test set. I problemi sorgono quando i dati non sono ben distribuiti.
	\item Split stratificato: si mescolano i dati prima di estrarli.
\end{itemize}

\clm{}{}{
	\begin{itemize}
		\item Ripetere la validazione Hold-Out più volte produrra risultati diversi poiché lo split è randomico.
		\item Per ottenere una stima migliore dell'errore di generalizzazione possiamo ripetere più volte e fare una media.
		\item La media di $n$ variabili indipendenti  è una nuova variabile avente media:
		      \[
			      \displaystyle\frac{1}{n}\displaystyle\sum_{i = 1}^{n} E_i = \displaystyle\frac{1}{n}\displaystyle\sum_{i = 1}^{n} \mu = \mu
		      \]
		\item E varianza:
		      \[
			      \displaystyle\frac{1}{n^2}\displaystyle\sum_{i = 1}^{n} \sigma^2 = \displaystyle\frac{\sigma^2}{n}
		      \]
	\end{itemize}
}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.55]{01/out.png}
	\caption{Validazione Hold-Out con differenti split ratio.}
\end{figure}

\nt{Se il dataset è abbastanza grande la validazione Hold-Out è semplice ed efficacie. Se il dataset è piccolo si ricorre alla \fancyglitter{Cross-Validation}.}

\dfn{Cross-Validation}{
	Ripetizione dell'Hold-Out validation $k$ volte, la prima volta si usa come test set $[0, \frac{1}{k}]$, la seconda $[\frac{1}{k}, \frac{2}{k}]$ fino all'ultima in cui si usa $[\frac{k-1}{k}, 1]$.
}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.55]{01/cross.png}
	\caption{Cross-Validation.}
\end{figure}

\qs{}{
	Perché usare la Cross-Validation invece di prendere la media degli Hold-Out ripetuti?
}

\begin{itemize}
	\item Nella Cross-Validation ogni esempio è usato come test esattamente una volta, mentre ripetendo Hold-Out alcuni esempi possono essere usati più volte e alcuni mai usati.
	\item È più accurata, ma se il dataset è sufficientemente grande la differenza è trascurabile.
\end{itemize}

\dfn{Leave-One-Out}{
	Ogni esempio è usato per testare esattamente una volra e tutti gli altri esempi sono usati per il training. L'errore di generalizzazione ha varianza molto bassa.
}

\nt{
	È lo stimatore più preciso, ma è estremamente costoso dal punto di vista computazionale.
}

\dfn{Bootstrapping}{
	Dato un dataset $D$ con $m$ esempi l'obiettivo è di stimare l'errore con l'intero dataset.  L'idea è quella di creare multipli esempi di bootstrap dal dataset originale per rimpiazzare gli esempi che verranno usati per stimare l'errore.
}

\nt{Gli esempi di bootstrap non sono disgiunti: diversi esempi saranno ripetuti e alcuni non compariranno.}

\paragraph{Probabilità che un esempio non sarà selezionato:}

$P(sns) = (1 - \displaystyle\frac{1}{m})^m$

\clm{}{}{
	\begin{itemize}
		\item Consideriamo $D'$ come bootstrap di $D$.
		\item Possiamo addestrare un modello su $D'$ e valutare la performance su $D - D'$.
		\item È particolarmente utile su piccoli dataset o quando non c'è un modo decente di splittare in training e dataset.
	\end{itemize}
}

\begin{itemize}
	\item \fancyglitter{Parametri:} gli oggetti modificati dall'algoritmo di apprendimento per adattarsi ai dati.
	\item \fancyglitter{Hyperparametri:} parametri dell'algoritmo di apprendimento in sè e per sè.
\end{itemize}

\dfn{Parameter Tuning}{
	Scegliere il miglior modello da un insieme di modelli candidati o scegliere i miglior Hyperparametri per un dato algoritmo.
}

\nt{
	Bisogna prestare attenzione a non fare overfitting sul test set.
}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.45]{01/tuning.png}
	\caption{Tuning.}
\end{figure}


\subsection{Misure di Performance}

Misure di performance differenti riflettono le varie domande di task e producono diversi risultati. Scegliere la misura giusta è cruciale per una valutazione equa del modello.

\dfn{Mean Square Error (MSE)}{
È una misura per task di regressione. Se si conosce la distribuzione dei dati di $D$:

\[
E(f; \mathcal{D})
= \mathbb{E}_{(x,y)\sim \mathcal{D}} \big[ (f(x) - y)^2 \big]
= \int_{(x,y)\sim \mathcal{D}} (f(x) - y)^2 p(x,y)\, dx\, dy
\]
}

\paragraph{Però spesso non si conosce la distribuzione, per cui si stima MSE come:}

\[
	E(f; D) = \frac{1}{m} \sum_{i=1}^m (f(x_i) - y_i)^2.
\]

\dfn{Error Rate}{
Assumendo nota la conoscenza della distribuzione $D$:
\[
E(f; D) = \mathbb{E}_{(\mathbf{x},y)\sim D} [\mathbb{I}(f(\mathbf{x}) \neq y)] = \int_{(\mathbf{x},y)\sim D} \mathbb{I}(f(\mathbf{x}) \neq y)p(\mathbf{x}, y)dx dy
\]
dove $\bbI$ è la funzione indicatore che restituisce 1 se la condizione è vera e 0 altrimenti.
}

\paragraph{Si può stimare come:}
\[
	E(f; D) = \frac{1}{m} \sum_{i=1}^{m} \mathbb{I}(f(\mathbf{x}_i) \neq y_i)
\]

\nt{L'accuratezza si calcola come complemento dell'error rate.}

\dfn{Matrice di Confusione}{
	Si tratta di una matrice 2x2 con esempi e predizioni (su righe o su colonne dipende da come gira all'autore).
}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{01/confusion.png}
	\caption{Matrice di confusione.}
\end{figure}

\paragraph{Altre misure:}

\begin{itemize}
	\item \fancyglitter{Precision:} l'abilità di identificare i casi positivi.
	      \begin{center}
		      Precision = $\frac{TP}{TP + FP}$
	      \end{center}
	\item \fancyglitter{Recall:} l'abilità di evitare di etichettare in modo sbagliato.
	      \begin{center}
		      Recall = $\frac{TP}{TP + FN}$
	      \end{center}
\end{itemize}

\clm{}{}{
	\begin{itemize}
		\item Precision e Recall sono spesso usate insieme perché complementari.
		\item Solitamente l'aumento di una implica la diminuzione dell'altra.
	\end{itemize}
}

\dfn{P-R Plots}{
	Assumendo di avere un classificatore che restituisce una misura di confidenza per ogni campione indicando quanto sia confidente che il campione appartenga alla classe positiva, s può variare un threshold per ottenere diversi valori di precision e recall.
}
\begin{figure}[h]

	\centering
	\includegraphics[scale=0.5]
	{01/pr.png}
	\caption{Esempio di P-R Plot.}
\end{figure}

\qs{}{Come selezioniamo il miglior modello date poche curve P-R?}

\begin{itemize}
	\item Fissare una data precision e prendere il modello con la maggiore recall (e viceversa).
	\item Prendere il miglior \fancyglitter{break-even point}: in cui precision e recall sono uguali.
\end{itemize}

\dfn{F1-Score}{
	Media armonizzata di precision e recall:
	\[
		F1 = \displaystyle\frac{2}{\displaystyle\frac{1}{\text{precision}} + \displaystyle\frac{1}{\text{recall}}}
	\]

}

\nt{Si può generalizzare come F$_\beta$-score in cui si dà un peso diverso a precision e recall.}

\paragraph{Per ottenere la media dei contributi di diverse matrici di confusione ci sono 2 modi:}

\begin{itemize}
	\item \fancyglitter{Macro-averaging:} si computa la precision (P) e la recall (R) per ogni matrice di confusione.
	      \[
		      \text{macro-}P = \frac{1}{n} \sum_{i=1}^{n} P_i
	      \]
	      \[
		      \text{macro-}R = \frac{1}{n} \sum_{i=1}^{n} R_i
	      \]
	      \[
		      \text{macro-}F_1 = \frac{2 \times \text{macro-}P \times \text{macro-}R}{\text{macro-}P + \text{macro-}R}
	      \]

	\item \fancyglitter{Micro-averaging:} si calcola la media degli elementi sulla matrice di confusione.
	      \[
		      \text{micro-}P = \frac{\overline{TP}}{\overline{TP} + \overline{FP}}
	      \]
	      \[
		      \text{micro-}R = \frac{\overline{TP}}{\overline{TP} + \overline{FN}}
	      \]
	      \[
		      \text{micro-}F_1 = \frac{2 \times \text{micro-}P \times \text{micro-}R}{\text{micro-}P + \text{micro-}R}
	      \]
\end{itemize}

\dfn{ROC Plot}{
	Invece di usare precision e recall si fa un plot dei false positive rate (FPR) contro il true positive rate (TPR) per ottenere la Receiver Operating Characteristic curve (ROC):
	\[
		FPR = \frac{FP}{FP + TN} = \frac{FP}{\# \text{ actual negatives}}
	\]
	\[
		TPR = \frac{TP}{TP + FN} = \frac{TP}{\# \text{ actual positives}}
	\]
}

\begin{figure}[h]

	\centering
	\includegraphics[scale=0.5]
	{01/roc.png}
	\caption{ROC Plot.}
\end{figure}

\begin{itemize}
	\item \fancyglitter{ROC Heaven:} classificatore perfetto, non sbaglia mai.
	\item \fancyglitter{ROC Hell:} classificatore che sbaglia sempre.
\end{itemize}

\nt{Avere un classificatore perfetto o uno che sbaglia sempre è la stessa cosa (basta prendere il contrario di cosa restituisce). Il punto peggiore è nel mezzo: tira sempre a caso.}

\paragraph{Area sotto la curva (AUC):}
\[
	1 - \text{AUC} = \ell_{\text{rank}} \triangle \frac{1}{m^+m^-} \sum_{\mathbf{x}^+ \in D^+} \sum_{\mathbf{x}^- \in D^-} \mathbb{I}[f(\mathbf{x}^+) < f(\mathbf{x}^-)] + \frac{1}{2}\mathbb{I}[f(\mathbf{x}^+) = f(\mathbf{x}^-)]
\]

\dfn{Cost-Sensitive Error Rate}{
	Il cost-sensitive error rate si calcola come:
	\[
		E(f; D; \text{cost}) = \frac{1}{m} \left( \sum_{(\mathbf{x},y) \in D^+} \mathbb{I}[f(\mathbf{x}) \neq y] \times \text{cost}_{01} + \sum_{(\mathbf{x},y) \in D^-} \mathbb{I}[f(\mathbf{x}) \neq y] \times \text{cost}_{10} \right)
	\]

	Dove si dà un costo agli errori.
}

\subsection{Comparison Test}

\paragraph{Confrontare i risultati di diversi classificatori non è triviale:}

\begin{itemize}
	\item Desideriamo valutare una performance generalizzata, ma abbiamo accesso solo a un insieme di test finito.
	\item Le performance del modello variano su insiemi di test diversi anche se hanno la stessa dimensione.
	\item La randomicità di diversi algoritmi di apprendimento possono produrre modelli diversi quando addestrati diverse volte sugli stessi dati.
\end{itemize}

\dfn{Binomial Testing}{
	Dato un modello $f$ con un error rate sconosciuto $\epsilon$ si desidera testare se il modello performi meglio di un dato threshold $\epsilon_0 \in [0, 1]$ avendo osservato il tasso di errore empirico $\hat \epsilon$ su un insieme di test di taglia $m$ per il modello $f$.
}

\paragraph{Hypothesis Test:}

\begin{itemize}
	\item \fancyglitter{Null Hypothesis:} il modello non è migliore di $\epsilon_0$.
	      \[H_0: \epsilon \ge \epsilon_0 \]
	\item \fancyglitter{Alternative Hypothesis:} il modello è migliore di $\epsilon_0$.
	      \[H_1: \epsilon < \epsilon_0 \]

	\item Assumendo $E$ come variabile casuale che conta il numero di errori commessi dal modello sull'insieme di test. Con $H_0$ abbiamo: $E \sim \text{Bin}(m, \epsilon_0)$.
\end{itemize}


\cor{Decision Rule}{
	Si rigetta l'ipotesi nulla $H_0$ a un livello $\alpha$ (con confidenza $1 - \alpha$) se:
	\[
		P(E \le \hat e | \epsilon = \epsilon_0) < \alpha
	\]
}

\nt{
	In altre parole: se è improbabile osservare poche errori concludiamo che il modello performi meglio di $H_0$.
}

\cor{P-Value}{
Il P-Value di un test statistico è la probabilità di osservare una statistica almeno estrema quanto quella effettivamente osservata, assumendo che l'ipotesi nulla sia vera.
\[
	\text{p-value} = P(E \le \hat{e} \mid \epsilon = \epsilon_0) = \sum_{k=0}^{\hat{e}} \binom{m}{k} \epsilon_0^k (1 - \epsilon_0)^{m-k}
\]

}

\cor{Critical Value}{
	Assumendo di voler ripetere un test più volte si può calcolare l'errore massimo per rigettare l'ipotesini nulla.
	\[
		\bar{\epsilon} = \underset{\epsilon}{\text{maximize}} \quad \epsilon
	\]
	\[
		\text{subject to} \quad \sum_{i=0}^{\lfloor \epsilon \times m \rfloor} \binom{m}{i} \epsilon^i (1-\epsilon)^{m-i} \leq \alpha
	\]
}

\dfn{Student's T-Test}{
	Ripetendo Hold-Out validation o Cross-Validation si possono calcolare:
	\begin{itemize}
		\item Media:
		      \[
			      \mu = \frac{1}{k} \sum_{i=1}^{k} \hat{\epsilon}_i
		      \]

		\item Varianza:
		      \[
			      \sigma^2 = \frac{1}{k} \sum_{i=1}^{k} (\hat{\epsilon}_i - \mu)^2
		      \]

	\end{itemize}

	E così facendo si può calcolare la distribuzione:
	\[
		\tau = \sqrt{k} \frac{\mu - \epsilon_0}{\sigma}
	\]

}

\clm{}{}{
	\begin{itemize}
		\item Il test di Student è usato per determinare se la media di due gruppi è significativamente diversa.
		\item Si può rigettare l'ipotesi nulla a un livello di significatività $\alpha$ se $\tau$ è al di fuori dell'intervallo $[-t_{\frac{\alpha}{2}}, t_{\frac{\alpha}{2}}]$.
	\end{itemize}
}

\begin{figure}[h]

	\centering
	\includegraphics[scale=0.3]
	{01/tst.png}
	\caption{Esempio di t di Student.}
\end{figure}

\cor{Cross-Validated Student's T-Test}{
	Assumendo di avere due classificatori $A$ e $B$ denotando con $\epsilon_1^A,..., \epsilon_k^A$ e $\epsilon_1^B,..., \epsilon_k^B$ gli error rate ottenuti dalla Cross-Validation, se le loro performance sono le stesse allora i testing error rates dovrebbero essere circa gli stessi: $\epsilon_i^A \approx \epsilon_i^B$.

}

\paragraph{Un metodo più robusto è 5x2 Cross-Validated T-Test:}

\begin{enumerate}
	\item Eseguire un 2-fold Cross-Validation, con la differenza tra errori $\Delta_{1,1}$ e $\Delta_{1,2}$.
	\item Ripetere il punto 1 quattro volte con mischiaggi randomici dei dati, in questo modo si ottenfono 5 paia differenti.
	\item Per ognuna si va a calcolare la varianza delle differenze:
	      \[
		      \sigma_i^2 = (\Delta_{i,1} - \bar{\Delta}_i)^2 + (\Delta_{i,2} - \bar{\Delta}_i)^2 \quad \text{where} \quad \bar{\Delta}_i = \frac{\Delta_{i,1} + \Delta_{i,2}}{2}
	      \]
	\item Si utilizza la differenza del primo fold\footnote{Per ammissione del professore nemmeno lui sa il perché.} e la media delle stime delle 5 varianze:
	      \[
		      \tau = \frac{\Delta_{1,1}}{\sqrt{\frac{1}{5} \sum_{i=1}^{5} \sigma_i^2}}
	      \]
	\item Sotto l'ipotesi nulla la statistica si distribuisce come una Student's T-Distribution con 5 gradi di libertà.
\end{enumerate}

\dfn{McNemar's Test}{
Assumendo di avere due classificatori $A$ e $B$ e volendo comparare le loro performance tramite Hold-Out si possono addestrare sullo stesso training set e valutarli sullo stesso test set. L'idea è che sotto l'ipotesi nulla il numero di errori fatto da ogni classificatore dovrebbe essere simile.
\[
	T_{\chi^2} = \frac{(|e_{10} - e_{01}| - 1)^2}{e_{10} + e_{01}},
\]

}

\paragraph{Decision Rule:}

\begin{itemize}
	\item Si rigetta l'ipotesi nulla a un livello di significatività $\alpha$ se:
	      \[
		      \tau_{\chi^2} > \chi_{\alpha,1}^2
	      \]
\end{itemize}

\nt{Ma sia il test di Student che quello di McNemar sono pensati per confrontare due algoritmi su un singolo dataset, però spesso si vogliono confrontare più algoritmi su più dataset.}

\dfn{Friedman's Test}{
	Si tratta di un ranking test che permette di confrontare più algoritmi su più dataset. Se l'ipotesi nulla è rigettata si effettua un test post-hoc (solitamente Nemenyi's Test) per identificare specifiche paia di algoritmi con performance molto diverse.
}

\paragraph{In pratica:}

\begin{itemize}
	\item Assumendo che si vogliano comparare gli algoritmi $A, B, C$ sui datasets $D_1, D_2, D_3$.
	\item Si inizia con un Hold-Out o una Cross-Validation per ottenere gli error rates di ogni algoritmo su ogni dataset.
	\item Si fa un ranking degli algoritmi per ogni dataset.
	      \begin{figure}[h]

		      \centering
		      \includegraphics[scale=0.4]
		      {01/friedman.png}
		      \caption{Esempio di Friedman's Test.}
	      \end{figure}
	\item Usiamo $k$ per definire il numero di algoritmi, $N$ per il numero dei datasets e $r_i$ la media dell'algoritmo $i$.
	\item Per calcolare la statistica del test di Friedman iniziamo dalla formula:
	      \[
		      \tau_{\chi^2} = \frac{12N}{k(k+1)} \left( \sum_{i=1}^{k} r_i^2 - \frac{k(k+1)^2}{4} \right)
	      \]

	\item Se ci fermassimo qua ci servirebbermo molti algoritmi (circa 100-150), per cui questa statistica è corretta da \fancyglitter{Iman} e \fancyglitter{Davenport}:
	      \[
		      \tau_F = \frac{(N-1)\tau_{\chi^2}}{N(k-1) - \tau_{\chi^2}}
	      \]
\end{itemize}

\cor{Nemenyi Post-Hoc Test}{
	Si cofrontano le performance dei classificatori guardando le differenze tra i loro ranks. La differenza è considerata significativamente differente se è maggiore di CD:
	\[
		CD = q_{\alpha} \sqrt{\frac{k(k+1)}{6N}}
	\]
	Dove:
	\begin{itemize}
		\item $k$ è il numero di algoritmi.
		\item $N$ è il numero di datasets.
		\item $q_\alpha$ è il critical value della distribuzione di Student a livello di significatività $\alpha$.
	\end{itemize}
}

\subsection{Decomposizione Bias-Varianza}

\dfn{Decomposizione Bias-Varianza}{
	Si tratta di un modo per capire meglio il motivo per cui un algoritmo produce un certo errore. Si tratta di 3 componenti:
	\begin{itemize}
		\item Bias: l'errore dovuto ad assunzioni sbagliate.
		\item Varianza: ci si sta adattando troppo al training set.
		\item Errori irriducibili: dovuti ai rumori nei dati.
	\end{itemize}
}

\paragraph{In pratica:}

\begin{itemize}
	\item Consideriamo un problema di regressione con $x$ come campione di test con etichetta $y_D$ nel dataset $D$, con $f(x;D)$ la predizione del modello $f$ addestrato su $D$.
	\item Il \fancyglitter{Valore atteso} è:
	      \[
		      \bar{f} = \mathbb{E}_D[f(\mathbf{x}; D)]
	      \]
	\item La \fancyglitter{Varianza} è:
	      \[
		      var = \mathbb{E}_D \left[ (f(\mathbf{x}; D) - \bar{f})^2 \right]
	      \]
	\item Il \fancyglitter{Rumore} è:
	      \[
		      \varepsilon^2 = \mathbb{E}_D \left[ (y_D - y)^2 \right]
	      \]
	\item Il \fancyglitter{Bias} è:
	      \[
		      bias^2 = (\bar{f} - y)^2
	      \]
\end{itemize}

\paragraph{Il valore atteso di MSE può essere scritto come:}

\begin{align*}
	E(f; D) & = \mathbb{E}_D[(f(\mathbf{x}; D) - y_D)^2]                                                                                                             \\
	        & = \mathbb{E}_D[(f(\mathbf{x}; D) - \overbrace{\bar{f} + \bar{f}}^{a} - \overbrace{y_D}^{b})^2] = E_D[(a + b)^2]                                        \\
	        & = \mathbb{E}_D[(f(\mathbf{x}; D) - \bar{f})^2] + \mathbb{E}_D[(\bar{f} - y_D)^2] + \cancel{2\mathbb{E}_D[(f(\mathbf{x}; D) - \bar{f})(\bar{f} - y_D)]} \\
	        & \quad \textcolor{red}{\text{since }} y_D \perp f_D                                                                                                     \\
	        & = \mathbb{E}_D[(f(\mathbf{x}; D) - \bar{f})^2] + \mathbb{E}_D[(\bar{f} - y_D)^2]                                                                       \\
	        & = var + \mathbb{E}_D[(\bar{f} - y + y - y_D)^2]                                                                                                        \\
	        & = var + \cancel{\mathbb{E}_D}[(\bar{f} - y)^2] + \mathbb{E}_D[(y - y_D)^2] + \cancel{2\mathbb{E}_D[(\bar{f} - y)(y - y_D)]}                            \\
	        & \quad \textcolor{red}{\text{since }} \mathbb{E}_D[y_D - y] = 0 \text{ } (\Rightarrow E_D[y_D] = y)                                                     \\
	        & = var + bias^2 + \varepsilon^2
\end{align*}

\clm{}{}{
	\begin{itemize}
		\item Il bias misura la capacità dell'algoritmo di adattarsi.
		\item La varianza misura l'impatto delle fluttuazioni nei dati sull'apprendimento.
		\item Il rumore rappresenta l'errore atteso per ogni algoritmo di apprendimento per un dato task.
	\end{itemize}
}

\nt{
	Per ottenere una performance eccellente un modello ha bisogno di un piccolo bias e di una piccola varianza.
}

\cor{Bias-Varianza Dilemma}{
	Trovare un trade-off tra bias e varianza, poiché generalmente un algoritmo con basso bias ha alta varianza e viceversa.
}

\section{Modelli Lineari}

\subsection{Least Squares}

\dfn{Modello Lineare}{
	Assumiamo $x = (x_1,..., x_d)^\top$ come campione descritto da $d$ variabili. Un modello lineare mira ad apprendere una funzione che consente di vare predizioni con una combinazione lineare delle variabili in input:
	\[
		f(\mathbf{x}) = w_1x_1 + w_2x_2 + \ldots + w_dx_d + w_{d+1} = \mathbf{w}^\top \mathbf{x} + w_{d+1}
	\]
	In cui  $\mathbf{w} = (w_1, w_2, \ldots, w_d)^\top$ è il vettore peso e $w_{d+1}$ è il termine di bias.
}

\nt{Il modello è determinato quando $w$ e $\mathbf{w}_{d+1}$ sono appresi dai dati.}

\dfn{Regressione Lineare}{
Dato un dataset $D = {(x_1, y_1), ..., (x_n,y_n)}$ dove $x_i = (x_{i1}, ..., x_{id})$ e $y_i \in \bbR$, la regressione lineare punta a imparare un modello lineare che possa predirre accuratamente delle vere etichette. Si cerca di ridurre la funzione:

\[
	\text{MSE}(\mathbf{w}, w_{d+1}) = \frac{1}{n} \sum_{i=1}^{n} (y_i - f(\mathbf{x}_i))^2 = \frac{1}{n} \sum_{i=1}^{n} (y_i - (\mathbf{w}^\top \mathbf{x}_i + w_{d+1}))^2
\]

}

\nt{La spiegazione di questo argomento è diversa da quella del libro.}

\dfn{Mean Square}{
	Si ha un dataset con un certo numero di punti e le loro coordinate. Si vuole trovare la retta che passi per tutti i punti.
}

\nt{È molto comoda la rappresentazione matriciale.}

\paragraph{Per esempio, il sistema:}

\[
	\begin{cases}
		w_1 \cdot (-1) + w_2 = -1.52   \\
		w_1 \cdot (-0.8) + w_2 = -1.21 \\
		w_1 \cdot (-0.6) + w_2 = -0.67 \\
		\ldots
	\end{cases}
\]

\paragraph{Può essere riscritto come \fancyglitter{matrice}:}

\[
	\begin{bmatrix}
		-1   \\
		-0.8 \\
		-0.6 \\
		\ldots
	\end{bmatrix}
	[w_1] + w_2 =
	\begin{bmatrix}
		-1.52 \\
		-1.21 \\
		-0.67 \\
		\ldots
	\end{bmatrix}
	= \mathbf{X}\mathbf{w} + w_2
\]

\cor{Coordinate Omogenee}{
	Incorporiamo il termine di bias $b$ nel vettore peso (che diventa $w^i = (w_1,..., w_d, w_{d+1}$) e aggiungiamo una colonna di 1 nella matrice X:
	\[
		\underset{\mathbf{X}}{\begin{bmatrix}
				-1     & 1 \\
				-0.8   & 1 \\
				-0.6   & 1 \\
				\ldots & 1
			\end{bmatrix}}
		\underset{\mathbf{w}}{\begin{bmatrix}
				w_1 \\
				w_2
			\end{bmatrix}}
		=
		\underset{\mathbf{y}}{\begin{bmatrix}
				-1.52 \\
				-1.21 \\
				-0.67 \\
				\ldots
			\end{bmatrix}}
		\equiv \mathbf{X}\mathbf{w} = \mathbf{y}
	\]
}

\ex{}{
	Dato l'esempio:
	\begin{align*}
		x_1 & = (1,2), \quad y_1 = 0.8 \\
		x_2 & = (3,4), \quad y_2 = 0.4 \\
		x_3 & = (1,4), \quad y_3 = 0.7 \\
		x_4 & = (2,2), \quad y_4 = 0.9
	\end{align*}
	Quali sono la matrice X e il vettore y?

	Nel caso ideale X è quadrata e l'equazione può essere semplicemente risolta moltiplicando entrambi i lati con l'inversa di X.
	\begin{align*}
		\mathbf{X}\mathbf{w}                                 & = \mathbf{y}                                 \\
		\Downarrow                                                                                          \\
		\textcolor{red}{\mathbf{X}^{-1}}\mathbf{X}\mathbf{w} & = \textcolor{red}{\mathbf{X}^{-1}}\mathbf{y} \\
		\Downarrow                                                                                          \\
		\mathbf{w}                                           & = \mathbf{X}^{-1}\mathbf{y}
	\end{align*}

	Sfortunatamente questo è vero solo nel caso triviale. X non è solitamente invertibile ed è necessario un approccio più generale.
}

\paragraph{Quindi, utilizzando un approccio geometrico:}

\begin{itemize}
	\item Iniziamo notando che variando w ci stiamo muovendo nello spazio delle colonne di X.
	      \begin{figure}[h]
		      \centering
		      \includegraphics[scale=0.35]{01/g1.png}
	      \end{figure}
	\item Dato che il sistema di equazioni non è risolvibile si capisce che nessuna combinazione di $w_i$ consente di ottenere y, per cui y non appartiene al piano C(X).
	      \begin{figure}[h]
		      \centering
		      \includegraphics[scale=0.35]
		      {01/g2.png}
	      \end{figure}
	\item Il miglior modo è quello di trovare il vettore w che minimizza la lunghezza del vettore errore $e = y - Xw$. In altre parole vogliamo trovare:
	      \[
		      \argmin_{\mathbf{w}} \|\mathbf{e}\| = \argmin_{\mathbf{w}} \|\mathbf{e}\|^2 = \argmin_{\mathbf{w}} \|\mathbf{y} - \mathbf{X}\mathbf{w}\|^2
	      \]
	      \begin{figure}[h]
		      \centering
		      \includegraphics[scale=0.35]{01/g3.png}
	      \end{figure}
	\item Come scegliamo il miglior punto sul piano $C(X)$ per approssimare y?
	\item Il miglior punto è la proiezione ortogonale di y su $C(X)$. Quel punto corrisponde a $p = Xw$ dove $w$ è il vettore peso ottimale.

	      \begin{figure}[h]

		      \centering
		      \includegraphics[scale=0.5]{01/g4.png}

	      \end{figure}
	\item Per imporre l'ortogonalità al piano si impone che sia ortogonale a tutti i vettori del piano:
	      \begin{align*}
		      \mathbf{x}_1 \perp \mathbf{e} & \Rightarrow \mathbf{x}_1^\top \mathbf{e} = 0 \\
		      \mathbf{x}_2 \perp \mathbf{e} & \Rightarrow \mathbf{x}_2^\top \mathbf{e} = 0 \\
		                                    & \ldots
	      \end{align*}
	\item Che può essere scritto come $X^\top e = 0$.
	\item La soluzione si può trovare risolvendo:
	      \begin{gather*}
		      \mathbf{X}^\top (\mathbf{y} - \mathbf{X}\mathbf{w}) = \mathbf{0} \\
		      \mathbf{X}^\top \mathbf{y} - \mathbf{X}^\top \mathbf{X}\mathbf{w} = \mathbf{0} \\
		      \mathbf{X}^\top \mathbf{X}\mathbf{w} = \mathbf{X}^\top \mathbf{y} \\
		      \mathbf{w} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}
	      \end{gather*}
\end{itemize}

\nt{
	Se i dati sono pochi o rumorosi la matrice $X^\top X$ può essere malcondizionata rendendo difficile l'inversione. In tali casi si effettuano tecniche di regolarizzazione per stabilizzare la soluzione.
}

\paragraph{Un'altra possibilità per derivare la soluzione di Least Squares è l'analisi:}

\begin{itemize}
	\item Possiamo riscrivere il problema come:
	      \begin{align*}
		      \argmin_{\mathbf{w}} \|\mathbf{y} - \mathbf{X}\mathbf{w}\|^2 & = \argmin_{\mathbf{w}} (\mathbf{y} - \mathbf{X}\mathbf{w})^\top (\mathbf{y} - \mathbf{X}\mathbf{w})                                                             \\
		                                                                   & = \argmin_{\mathbf{w}} \left( \mathbf{y}^\top \mathbf{y} - 2\mathbf{y}^\top \mathbf{X}\mathbf{w} + \mathbf{w}^\top \mathbf{X}^\top \mathbf{X}\mathbf{w} \right)
	      \end{align*}
	\item Differenziando rispetto a w e impostando la derivata a 0 otteniamo:
	      \begin{align*}
		      \frac{\partial}{\partial \mathbf{w}} \left( \mathbf{y}^\top \mathbf{y} - 2\mathbf{y}^\top \mathbf{X}\mathbf{w} + \mathbf{w}^\top \mathbf{X}^\top \mathbf{X}\mathbf{w} \right) & = 0                                                            \\
		      -2\mathbf{X}^\top \mathbf{y} + 2\mathbf{X}^\top \mathbf{X}\mathbf{w}                                                                                                          & = 0                                                            \\
		      \mathbf{X}^\top \mathbf{X}\mathbf{w}                                                                                                                                          & = \mathbf{X}^\top \mathbf{y}                                   \\
		      \mathbf{w}                                                                                                                                                                    & = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}
	      \end{align*}
	\item Dove:
	      \[
		      \frac{\partial}{\partial \mathbf{w}} \left[ \mathbf{w}^\top \mathbf{X}^\top \mathbf{X}\mathbf{w} \right] = 2\mathbf{X}^\top \mathbf{X}\mathbf{w}
	      \]
	\item E:
	      \[
		      \frac{\partial}{\partial \mathbf{w}} \left[ \mathbf{y}^\top \mathbf{X}\mathbf{w} \right] = \mathbf{X}^\top \mathbf{y}
	      \]
\end{itemize}

\subsection{Regolarizzazione}

Il metodo Least Squares è uno strumento potente, ma non è immune all'overfitting.

\dfn{Regolarizzazione}{
	La regolarizzazione è un metodo generale per evitare l'overfitting applicando vincoli aggiuntivi al vettore peso. Un approccio comune consiste nell'assicurarsi che i pesi sono piccoli in magnitudo (shrinkage).
}

\paragraph{La versione regolarizzata del Least Squares è:}
\[
	\underset{\mathbf{w}}{\text{arg min}} \|\mathbf{y} - \mathbf{X}\mathbf{w}\|^2 + \lambda \|\mathbf{w}\|_p^p
\]

\nt{Dove $\lambda$ è un Hyperparametro che controlla la quantità di regolarizzazione e $p$ è la norma usata per misurare la dimensione dei pesi.}

\cor{Ridge Regression}{
	Nel caso di norma $L_2$ la versione regolarizzata di Least Squares ha una soluzione in forma chiusa:
	\[
		\hat{\mathbf{w}} = (\mathbf{X}^\top \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^\top \mathbf{y}
	\]
	Dove $I$ è la matrice identità.
}

\clm{}{}{
	\begin{itemize}
		\item Una matrice è invertibile se e solo se tutti i suoi autovalori sono non-zero.
		\item Aggiungere $\lambda$ ci assicura che tutti gli autovalori sono reali e non-negativi.
	\end{itemize}
}

\cor{Lasso Regression}{
	Nel caso di norma $L_1$:
	\[
		\|\mathbf{w}\|_1 = \sum_{i} |w_i|
	\]
	Il risultato è che alcuni pesi sono ridotti e altri settati a 0.
}

\nt{Lasso Regression favorisce soluzioni sparse.}
\begin{figure}[h]

	\centering
	\includegraphics[scale=0.5]{01/reg.png}
	\caption{Confronti tra regressioni.}
\end{figure}

\qs{}{
	Perché i metodi di regolarizzazione funzionano?
}

\begin{itemize}
	\item Se si assume che X è affetta da un errore D allora: (X + D)w = Xw + Dw.
	\item Per cui minimizzare la norma del vettore peso minimizza gli effetti dell'errore su X.
	\item Limitando i pesi introduciamo un bias verso modelli più semplici riducendo la varianza:
	      \begin{itemize}
		      \item Una grande riduzione nella varianza compensa un piccolo incremento nel bias, riducendo gli errori.
		      \item Possiamo utilizzare il rasosio di Occam, assumendo che modelli più semplici generalizzano meglio.
	      \end{itemize}
\end{itemize}


\subsection{Generalizzare Modelli Lineari e Regressioni Logistiche}



