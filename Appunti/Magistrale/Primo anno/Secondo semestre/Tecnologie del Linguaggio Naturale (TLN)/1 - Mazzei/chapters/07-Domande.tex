\chapter{Domande per l'Esame}

\qs{}{Come funziona il Transition Based Parser (MALT)?}

\paragraph{Risposta:} si tratta di un tipo di parser che utilizza una grammatica a dipendenze, un algoritmo di ricerca bottom-up e depth-first con oracolo probabilistico.  

\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.45]{03/parserf.png}
    \caption{Transition-based parsing.}
\end{figure}

\begin{itemize}
  \item \fancyglitter{Stack:} contiene i nodi parzialmente analizzati.
  \item \fancyglitter{Input Buffer:} lista ordinata di parole fornite in input.
  \item \fancyglitter{Dependency Relations:} insieme delle dipendenze analizzate. 
  \item \fancyglitter{Parser:} guarda l'input, guarda sullo stack ed effettua una di tre possibili azioni (operazioni di transizione tra stati):
    \begin{itemize}
      \item LEFTARC: crea un arco a sinistra (il top dello stack e la seconda parola sullo stack). Rimuove la seconda parola dallo stack. 
      \item RIGHTARC: crea un arco a destra (tra la seconda parola sullo stack e il top dello stack). Rimuove il top dello stack.
      \item SHIFT: prende la parola in input e la mette nel top dello stack.
    \end{itemize}
\end{itemize}

\cor{Proiettività}{
  IF $i \rightarrow j$ THEN $i \rightarrow * k$ for any $k$ such that $i<k<j$ or $j<k<i$.
}

\nt{In parole povere: se non ci sono incroci.}

\paragraph{Stati:}

\begin{itemize}
  \item \fancyglitter{Stato iniziale:} \{$[$root$]$,$[$sentence$]$, ()\}
  \item \fancyglitter{Stato finale:} \{$[$root$]$,$[$$]$, (R)\} 
    \begin{itemize}
      \item R è l'insieme delle dipendenze costruite. 
      \item  $[]$ è la lista vuota perché tutte le parole sono state analizzate.
    \end{itemize}

\end{itemize}

\paragraph{L'algoritmo:}

\begin{itemize}
  \item Finché lo stato non è finale: 
    \begin{itemize}
      \item Un operatore viene scelto dall'oracolo probabilistico. 
      \item Viene applicato l'operatore effettuando un cambiamento dello stato.
    \end{itemize}
  \item Viene restituito lo stato.
\end{itemize}

\nt{Algoritmo greedy.}

\paragraph{Problemi:}

\begin{itemize}
  \item Tipare le relazioni: aggiungere le relazioni agli operatori, in questo modo si passa da 3 operatori a $2n + 1$ dove $n$ è il numero di relazioni. 
  \item Come capire quale operatore utilizzare: si potrebbe usare un oracolo a regole, ma non è praticabile. Quindi si sceglie si utilizzare un sistema di ML per addestrare un classifier per scegliere l'operatore $\Rightarrow$ si riduce il parsing a un task di classificazione.
\end{itemize}

\qs{}{Cosa è il task della Referring Expression Generation?}

\paragraph{Risposta:} si tratta del task per cui si vuole determinare quali parole usare per esprimere le entità del dominio (nomi propri e pronomi). Spesso si fanno parafrasi per bilanciare fluentezza e ambiguità. 

\qs{}{Spiegare l'uso delle HMM nel PoS Tagging.}

\paragraph{Risposta:} le Hidden Markov Models sono modelli probabilistici utilizzati nel PoS tagging. Sono composte da:

\begin{itemize}
  \item \fancyglitter{Stati nascosti (hidden state):} i tag PoS. 
  \item \fancyglitter{Osservazioni:} le parole nel testo. 
  \item \fancyglitter{Probabilità di transizione:} probabilità che un tag segua un altro. 
  \item \fancyglitter{Probabilità di emissione:} probabilità che una parola appaia dato un tag.
\end{itemize}

\paragraph{Modelling:}

\begin{itemize}
  \item L'obiettivo è trovare la sequenza di tag $\hat{t}_1^n$ che massimizza la probabilità a posteriori:
  \[
  \hat{t}_1^n = \operatorname{argmax}_{t_1^n} P(t_1^n|w_1^n)
  \]
  dove:
  \begin{itemize}
    \item $\hat{t}_1^n$ è la stima della sequenza ottimale di tag
    \item $w_1^n$ rappresenta la sequenza di parole osservate
  \end{itemize}

  \item Applicando il teorema di Bayes e le proprietà delle HMM:
  \[
  P(t_1^n|w_1^n) \propto P(w_1^n|t_1^n) \cdot P(t_1^n) = \prod_{i=1}^n P(w_i|t_i) \cdot P(t_i|t_{i-1})
  \]
  con $t_0 = \text{\texttt{START}}$ (stato iniziale).

  \item Notazioni chiave:
  \begin{itemize}
    \item $\hat{t}$ (hat): stima della migliore sequenza
    \item $\operatorname{argmax}_x f(x)$: valore di $x$ che massimizza $f(x)$
  \end{itemize}

  \item Esempio concreto per una frase di 3 parole:
  \[
  \hat{t}_1^3 = \operatorname{argmax}_{t_1,t_2,t_3} P(t_1)P(t_2|t_1)P(t_3|t_2) \cdot P(w_1|t_1)P(w_2|t_2)P(w_3|t_3)
  \]
\end{itemize}
\paragraph{Learning (Apprendimento dei parametri):}
\begin{itemize}
    \item \fancyglitter{Probabilità di transizione:} 
    $$P(tag_i | tag_{i-1}) = \frac{\text{Conteggio}(tag_{i-1} \rightarrow tag_i)}{\text{Conteggio}(tag_{i-1})}$$
    Esempio: Se \texttt{DET} è seguito da \texttt{NOUN} 70 volte su 100, allora $P(\texttt{NOUN}|\texttt{DET}) = 0.7$.

    \item \fancyglitter{Probabilità di emissione:} 
    $$P(parola | tag) = \frac{\text{Conteggio}(parola \text{ con } tag)}{\text{Conteggio}(tag)}$$
    Esempio: Se la parola "cane" appare 90 volte come \texttt{NOUN}, $P(\texttt{"cane"}|\texttt{NOUN}) = 0.9$.

    \item \fancyglitter{Probabilità iniziali:} 
    $$P(tag_1) = \frac{\text{Conteggio}(tag_1 \text{ come primo tag})}{\text{Totale frasi}}.$$
\end{itemize}

\paragraph{Decoding (Assegnazione dei tag):}
L'algoritmo di Viterbi seleziona la sequenza di tag $\hat{t}_1^n$ che massimizza:
$$\argmax_{t_1^n} P(t_1^n | w_1^n) \propto \prod_{i=1}^n P(t_i | t_{i-1}) \cdot P(w_i | t_i),$$
dove:
\begin{itemize}
    \item $t_i$ è il tag alla posizione $i$,
    \item $w_i$ è la parola alla posizione $i$.
\end{itemize}

\qs{}{Che differenza c'è tra MEMM e HMM?}

\paragraph{Risposta:} I modelli MEMM (Maximum Entropy Markov Models) e HMM (Hidden Markov Models) sono approcci probabilistici diversi per il tagging sequenziale. Le differenze principali sono:

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Aspetto} & \textbf{HMM} & \textbf{MEMM} \\
\hline
\fancyglitter{Modello probabilistico} & Generativo & Discriminativo \\
\hline
\fancyglitter{Probabilità modellate} & $P(W,T) = \prod P(w_i|t_i)P(t_i|t_{i-1})$ & $P(T|W) = \prod P(t_i|t_{i-1},w_i)$ \\
\hline
\fancyglitter{Feature} & Solo parole e tag & Feature arbitrarie (es: prefissi, suffissi) \\
\hline
\fancyglitter{Problema noto} & Independence assumptions & Label bias problem \\
\hline
\fancyglitter{Efficienza} & $O(nT^2)$ (Viterbi) & $O(nT^2)$ (Viterbi modificato) \\
\hline
\end{tabular}
\end{center}

\paragraph{Dettagli chiave:}

\begin{itemize}
\item \fancyglitter{Natura dei modelli:}
\begin{itemize}
\item Gli HMM sono \textit{generativi}: modellano la probabilità congiunta $P(W,T)$
\item I MEMM sono \textit{discriminativi}: modellano direttamente $P(T|W)$
\end{itemize}

\item \fancyglitter{Flessibilità:}
\begin{itemize}
\item Gli HMM usano solo emissioni $P(w|t)$ e transizioni $P(t_i|t_{i-1})$
\item I MEMM possono incorporare feature complesse (es: "La parola finisce con '-mente'")
\end{itemize}

\item \fancyglitter{Problemi intrinseci:}
\begin{itemize}
\item HMM soffre di \textit{independence assumptions} (emissioni indipendenti dal contesto)
\item MEMM soffre di \textit{label bias} (tendenza a preferire stati con meno transizioni)
\end{itemize}

\item \fancyglitter{Esempio di feature in MEMM:}
\[
P(t_i|t_{i-1},w_i) = \frac{1}{Z}\exp\left(\sum_j \lambda_j f_j(t_i,t_{i-1},w_i)\right)
\]
dove $f_j$ sono feature (es: "la parola è maiuscola?") e $\lambda_j$ pesi appresi.
\end{itemize}

\qs{}{Dove si trovano le lingue naturali nella gerarchia di Chomsky?}

\paragraph{Risposta:}
La \textbf{gerarchia di Chomsky} è una classificazione delle grammatiche formali basata sulla loro potenza espressiva. È suddivisa in quattro livelli principali:

\begin{enumerate}
    \item \textbf{Tipo 3 — Regolari}: le più semplici. Generano linguaggi come $a^n$, descrivibili con automi a stati finiti (DFA/NFA).
    \item \textbf{Tipo 2 — Context-Free (CF)}: possono generare strutture nidificate come $a^n b^n$. Sono riconosciute da \textit{automi a pila} (PDA) e descritte da grammatiche context-free (CFG).
    \item \textbf{Tipo 1 — Context-Sensitive (CS)}: più potenti, possono descrivere linguaggi come $a^n b^n c^n$ o $a^{2^n}$. Richiedono \textit{macchine a nastro linearemente limitato}.
    \item \textbf{Tipo 0 — Ricorsivamente Enumerabili}: le più generali, corrispondono a ciò che può essere calcolato da una \textit{macchina di Turing}. Possono generare anche linguaggi non decidibili.
\end{enumerate}

\medskip

Le \textbf{lingue naturali} non sono completamente catturate da nessuno di questi livelli in modo perfetto, ma studi linguistici (in particolare Shieber, Joshi, ecc.) hanno mostrato che esse richiedono una potenza espressiva maggiore delle context-free ma inferiore alle context-sensitive. Per questo motivo, si collocano in una classe intermedia nota come:

\begin{center}
    \fancyglitter{Mildly Context-Sensitive Languages (MCSLs)}
\end{center}

\medskip

Le MCSLs sono in grado di generare strutture come:

\begin{itemize}
    \item $a^n b^n c^n$ (non generabile da una CFG)
    \item Dipendenze incrociate come quelle presenti in alcune lingue germaniche (es. olandese o tedesco)
\end{itemize}

\noindent
Queste lingue sono gestibili da formalismi come:
\begin{itemize}
    \item Tree-Adjoining Grammars (TAG)
    \item Linear Indexed Grammars
    \item Combinatory Categorial Grammars (CCG)
\end{itemize}

\medskip

\noindent
\textbf{In sintesi:} le lingue naturali si trovano in una classe intermedia tra le grammatiche context-free (Tipo 2) e le context-sensitive (Tipo 1), e sono descritte come \textit{Mildly Context-Sensitive Languages}.

\qs{}{Cosa è una probabilistic CFG?}

\paragraph{Risposta:}
Una \textbf{Probabilistic Context-Free Grammar (PCFG)} è una grammatica context-free in cui ogni \textit{produzione} è associata a una \textbf{probabilità}.

\medskip
Formalmente, una PCFG è una 5-upla $\langle N, \Sigma, R, S, P \rangle$, dove:
\begin{itemize}
    \item $N$ è l'insieme dei simboli non terminali,
    \item $\Sigma$ è l'insieme dei simboli terminali,
    \item $R$ è l'insieme delle regole di produzione del tipo $A \rightarrow \alpha$,
    \item $S \in N$ è il simbolo iniziale,
    \item $P$ è una funzione di probabilità definita su ogni regola $A \rightarrow \alpha$.
\end{itemize}

\medskip
La funzione $P$ assegna a ogni regola una probabilità tale che:

\[
\sum_{\{A \rightarrow \alpha \in R\}} P(A \rightarrow \alpha) = 1 \quad \text{per ogni } A \in N
\]

Ovvero: \textit{la somma delle probabilità di tutte le produzioni con lo stesso lato sinistro dev'essere 1}.

\medskip
\noindent
Le PCFG sono utilizzate per:
\begin{itemize}
    \item modellare l'\textbf{ambiguità} nelle grammatiche (scegliere l'albero più probabile tra molti),
    \item effettuare il \textbf{parsing probabilistico} in NLP,
    \item addestrare grammatiche da corpus annotati.
\end{itemize}

\medskip
\noindent
\textbf{Esempio:}

\begin{align*}
S &\rightarrow NP\;VP \quad\quad\quad P = 1.0 \\
VP &\rightarrow V\;NP \quad\quad\quad P = 0.5 \\
VP &\rightarrow V \quad\quad\quad\;\;\;\;\; P = 0.5
\end{align*}

\noindent
Qui, la probabilità di ciascuna regola è usata per calcolare la probabilità totale di un dato albero sintattico (moltiplicando le probabilità delle regole applicate).


\qs{}{Che cosa è il realizer nella NLG?}

\paragraph{Risposta:} Il \fancyglitter{realizer} (o \textit{surface realizer}) è il componente fondamentale di un sistema di Natural Language Generation (NLG) che trasforma una rappresentazione astratta del contenuto (tipicamente strutture sintattiche o semantiche) in testo linguisticamente corretto e fluente. 

\paragraph{Funzionamento del realizer:}
\begin{itemize}
  \item \fancyglitter{Input:} Riceve una rappresentazione intermedia (es: \textit{deep syntax} o \textit{semantic graphs})
  \item \fancyglitter{Output:} Genera frasi grammaticalmente corrette nella lingua target
  \item \fancyglitter{Processi chiave:}
  \begin{itemize}
    \item \textbf{Linearizzazione}: Disposizione ordinata delle parole
    \item \textbf{Morfologia}: Flessione di genere, numero, tempo verbale
    \item \textbf{Surface realization}: Scelta di articoli/preposizioni (es: "al" vs "allo")
    \item \textbf{Orthographic realization}: Capitalizzazione, punteggiatura
  \end{itemize}
\end{itemize}

\paragraph{Esempio concreto:}
\begin{itemize}
  \item \textbf{Input}: 
  \begin{verbatim}
    (AGGIUNGI (OGGETTO "libro") 
    (ATTRIBUTI (COLORE "rosso") (POSIZIONE "scrivania"))
  \end{verbatim}
  \item \textbf{Output}: \textit{"Aggiungi il libro rosso sulla scrivania"}
\end{itemize}

\paragraph{Tecnologie utilizzate:}
\begin{itemize}
  \item \fancyglitter{Grammar-based}: Realizer con grammatiche hand-crafted (es: FUF/SURGE)
  \item \fancyglitter{Statistical}: Modelli neurali (LSTM/Transformer) addestrati su corpora
  \item \fancyglitter{Hybrid}: Combinazione di regole e modelli statistici
\end{itemize}

\paragraph{Strumenti noti:}
\begin{itemize}
  \item \textbf{SimpleNLG}: Libreria Java per realizzazione in inglese/francese
  \item \textbf{GRAMLEX}: Sistema per flessione morfologica italiana
  \item \textbf{OpenCCG}: Realizer basato su grammatiche categoriali
\end{itemize}

\qs{}{Cosa è il task dell'Aggregation?}

\paragraph{Risposta:} L'\fancyglitter{Aggregation} è un task fondamentale nella Natural Language Generation (NLG) che combina multiple unità d'informazione in strutture linguistiche più complesse e naturali, riducendo la ridondanza e migliorando la fluidità del testo generato.

\qs{}{Elencare i 5 algoritmi per il parsing delle dipendenze sintattiche.}

\paragraph{Risposta:}

\begin{itemize}
  \item Dynamic Programming (simile a CKY): Ha complessità $O(n^5)$, ma Eisner ne ha scoperto uno con complessità $O(n^3)$. 
  \item Graph Algorithm: si crea un minimum spanning tree e vengono valutate le dipendenze in modo indipendente utilizzando un ML classifier. 
  \item Constituency Parsing: si fa il parsing con una grammatica a costituenti e si converte in una grammatica a dipendenze attraverso una \fancyglitter{tabella di percolazioni}.
  \item Transition-based parsing (MALT): anche detto deterministico. Si fanno scelte greedy per la creazione di dipendenze tra parole, guidate da ML classifier.
  \item Constraint Satisfaction: vengono eliminate tutte le possibili dipendenze che non soddisfano certi vincoli (un esempio è TUP).
\end{itemize}

\qs{}{Quali sono i task della NLG?}

\paragraph{Risposta:}

\begin{enumerate}
  \item \fancyglitter{Content determination:} 
    \begin{itemize}
      \item Messaggi provenienti da una struttura dati. 
      \item I messaggi sono aggregazioni di dati che possono essere espressi linguisticamente, con una parola o un sintagma. 
      \item I messaggi sono basati su entità, concetti e relazioni sul dominio. Non c'è nulla di linguistico.  
      \item \evidence{Esempio:} IDENTITY(NEXTTRAIN, CALEDONIANEXPRESS); The next train is the Caledonian Express.
      \item \evidence{Esempio:} DEPARTURETIME(CALEDONIANEXPRESS, 1000); The Caledonian Express leaves at 10am. 
      \item \evidence{Esempio:} COUNT((TRAIN, SOURCE(ABERDEEN),DESTINATION(GLASGOW)), 20, PERDAY); There are 20 trains daily from Aberdeen to Glasgow.
    \end{itemize}
  \item \fancyglitter{Discourse planning:}
    \begin{itemize}
      \item Un testo non è solo un collezione casuale di frasi. 
      \item C'è una struttura che relaziona le frasi. 
      \item Due tipi di relazione:
        \begin{itemize}
          \item Raggrupamento concettuale. 
          \item Relazioni retoriche.
        \end{itemize}
      \item \evidence{Esempio:}
\begin{figure}[h]
    \centering
    \includegraphics[scale=0.4]{06/dp.png}
\end{figure}

    \end{itemize}
  \item \fancyglitter{Sentence Aggregation:}
    \begin{itemize}
      \item Se si dicessero tutte le frasi 1-a-1 si avrebbe un discorso poco naturale e non fluente. 
      \item Per questo motivo i messaggi vengono combinati per formare frasi più complesse. 
      \item Può causare ambiguità, ma ci si aspetta che l'altro sappia disambiguare.
      \item È il primo task che dipende dalla lingua.
      \item \evidence{Esempio:} The next train, which leaves at 10 am, is the Caledonian Express.
    \end{itemize}
  \item \fancyglitter{Lexicalization:} 
    \begin{itemize}
      \item Determina: 
        \begin{itemize}
          \item Quali parole usare per esprimere i concetti e le relazioni del dominio. 
          \item Quali relazioni sintattiche tra le parole.
        \end{itemize}
      \item \evidence{Esempio:} si deve usare partire o decollare?
    \end{itemize}
  \item \fancyglitter{Referring Expression Generator:}
    \begin{itemize}
      \item Generare le “referring expressions” determina quali parole usare per esprimere le entità del dominio in una maniera comprensibile all'utente. 
      \item Si bilancia fluentezza e ambiguità (a volte con parafrasi).
      \item \evidence{Esempio:} 
        \begin{itemize}
          \item Aberdeen, Scotland. 
          \item Aberdeen.
        \end{itemize}
      \item \evidence{Esempio:}
        \begin{itemize}
          \item The train that leaves at 10am. 
          \item The next train.
        \end{itemize}
    \end{itemize}
  \item \fancyglitter{Syntactic and morphological realization:}
    \begin{itemize}
      \item Ogni lingua ha: 
        \begin{itemize}
          \item Morfologia: come si formano le parole. 
          \item Sintassi: come si formano le frasi.
        \end{itemize}
      \item Arrivati a questo livello il dominio non ha più alcuna importanza, è tutta linguistica.
    \end{itemize}
  \item \fancyglitter{Orthographic realization:} 
    \begin{itemize}
      \item Lettere grandi. 
      \item Punteggiatura. 
      \item Font. 
      \item Impaginazione.
    \end{itemize}
\end{enumerate}


\qs{}{Spiegare l'uso del formalismo lambda-FoL per la semantica computazionale.}

\paragraph{Risposta:} si usa la logica del prim'ordine (FoL) per poter ragionare sul linguaggio in maniera automatica. Tuttavia la FoL non permette di rappresentare alcune cose (predicati universali), per cui si introduce la lambda astrazione. Il motivo è che serve poter avere delle funzioni.

\nt{Maggiori informazioni nell'apposita sezione.}

\qs{}{Cosa è la rappresentazione Neo-Davidsoniana?}

\paragraph{Risposta:} una rappresentazione che usa FoL in cui si reificano gli eventi, ossia si definisce una variabile per identificare l'evento. Inoltre si distinguono i ruoli semantici (agente, paziente, etc.).

\qs{}{Quali sono le due ipotesi fondamentali della linguistica computazionale?}

\paragraph{Risposta:} La linguistica computazionale si fonda su due ipotesi fondamentali che guidano l'analisi automatica del linguaggio naturale:

\begin{itemize}
  \item Ipotesi dei livelli: l'analisi del linguaggio si fa a livelli. 
    
\begin{enumerate}
  \item Fonetica e Fonologia: lo studio del suono della lingua. 
  \item Morfologia: lo studio delle parti significative delle parole. 
  \item Sintassi: lo studio sulla struttura e sulle relazioni tra le parole. 
  \item Semantica: lo studio del significato. 
  \item Pragmatica: lo studio di come il linguaggio è usato per compiere goal. Il passivo serve per mettere in luce/enfatizzare alcune parti della frase. 
  \item Discorso: lo studio delle unità linguistiche rispetto alla singola dichiarazione.
    \end{enumerate}
  \item Ipotesi sequenziale: ogni livello è collegato al successivo. Ogni livello prende in input l'output del livello precedente. 

\end{itemize}
\qs{}{Cosa è la frame-based semantic nei DS?}

\paragraph{Risposta:} in un dialogue system si hanno dei frames che vanno riempiti con informazioni che vanno ricavate dal dialogo con l'utente. 

\nt{Vedere apposita sezione per più dettagli.}

\qs{}{Fare un esempio di ambiguità puramente semantica.}

\paragraph{Risposta:} ogni uomo ama una donna. 

\begin{itemize}
  \item Senso 1: ogni uomo ama una singola donna. 
  \item Senso 2: ogni uomo ama la sua donna.
\end{itemize}

\nt{Esempio di merda, ma non avevo voglia di inventarne uno.}

\qs{}{Ambiguità sintattica: spiegare i fenomeni del PP attachment e dell'ambiguità sintattica.}

\paragraph{Risposta:} L'\fancyglitter{ambiguità sintattica} si verifica quando una frase ammette molteplici analisi strutturali, con significati potenzialmente diversi. Una PP può modificare diversi costituenti.

\paragraph{Esempio canonico:}
\textit{"Ho visto l'uomo con il telescopio"}

\begin{itemize}
  \item \fancyglitter{Interpretazione 1} (attacco al VP):
  \begin{itemize}
    \item Struttura: [VP [V visto] [NP l'uomo] [PP con il telescopio]]
    \item Significato: L'osservatore usa il telescopio
    \item Rappresentazione: $\lambda e.\,\text{vedere}(e) \land \text{Agente}(e,\text{io}) \land \text{Strumento}(e,\text{telescopio})$
  \end{itemize}

  \item \fancyglitter{Interpretazione 2} (attacco all'NP):
  \begin{itemize}
    \item Struttura: [VP [V visto] [NP l'uomo [PP con il telescopio]]]
    \item Significato: L'uomo possiede il telescopio
    \item Rappresentazione: $\lambda e.\,\text{vedere}(e) \land \text{Agente}(e,\text{io}) \land \text{Possessore}(\text{uomo},\text{telescopio})$
  \end{itemize}
\end{itemize}
\qs{}{Come funziona l'algoritmo CKY?}

\nt{Vedere l'apposità sezione.}

\qs{}{Qual è la complessità dell'algoritmo CKY rispetto alla lunghezza $n$ dell'input?}

\paragraph{Risposta:}
L'algoritmo CKY (Cocke–Kasami–Younger), applicato a grammatiche in forma normale di Chomsky (CNF), ha una complessità temporale di:

\begin{itemize}
    \item $\mathcal{O}(n^3)$ nella versione \textbf{non probabilistica}, in cui si verifica soltanto se una frase è derivabile dalla grammatica e, opzionalmente, si costruisce un albero di parsing.
    \item $\mathcal{O}(n^5)$ nella versione \textbf{probabilistica} (PCFG), dove si tiene conto anche delle probabilità delle regole e si calcola il parse tree con probabilità massima. Questo avviene perché, per ogni cella della tabella, è necessario esplorare tutte le combinazioni di suddivisioni e accumulare le probabilità.
\end{itemize}

\medskip
\noindent
In entrambi i casi, l'algoritmo richiede anche spazio $\mathcal{O}(n^2)$ per la tabella triangolare che memorizza i risultati intermedi.


\qs{}{Che differenza c'è tra la sintassi formalizzata come \textit{dipendenze} e quella come \textit{costituenti}?}

\paragraph{Risposta:}  
La sintassi a \textbf{costituenti} (o \textit{sintassi a frasi}) e quella a \textbf{dipendenze} rappresentano due approcci distinti per analizzare la struttura sintattica di una frase.

\begin{itemize}
    \item \textbf{Sintassi a costituenti}: rappresenta la frase come un \textbf{albero gerarchico} formato da \textit{costituenti}, ossia gruppi di parole che funzionano come unità (es. sintagma nominale \texttt{NP}, sintagma verbale \texttt{VP}, ecc.). Ogni costituente può contenere altri costituenti più piccoli, fino ad arrivare alle parole.
    
    \begin{center}
    \begin{minipage}{0.8\textwidth}
    \begin{verbatim}
[S
  [NP Il gatto]
  [VP dorme [PP sul tappeto]]
]
    \end{verbatim}
    \end{minipage}
    \end{center}
    
    Questo tipo di analisi è tipico delle grammatiche costituenziali, come le \textit{Context-Free Grammars}.

    \item \textbf{Sintassi a dipendenze}: rappresenta la frase come un insieme di \textbf{relazioni binarie} tra parole, dove ogni parola (eccetto la radice) \textit{dipende} da un'altra. La struttura è un grafo orientato, spesso ad albero, in cui i legami indicano ruoli grammaticali (soggetto, oggetto, ecc.).

    \begin{center}
    \begin{minipage}{0.8\textwidth}
    \begin{verbatim}
dorme  →  soggetto  →  gatto
dorme  →  complemento  →  tappeto
tappeto  →  determinante  →  il
dorme  →  preposizione  →  su
    \end{verbatim}
    \end{minipage}
    \end{center}

    Questo tipo di rappresentazione è comune nella linguistica computazionale e nel parsing NLP (es. Universal Dependencies).
\end{itemize}

\medskip
\noindent
\textbf{Differenze principali:}

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Aspetto} & \textbf{Costituenti} & \textbf{Dipendenze} \\
\hline
Unità di base & Gruppi di parole (sintagmi) & Relazioni tra parole \\
Struttura & Albero gerarchico & Grafo di dipendenze \\
Focus & Composizione frasale & Funzione grammaticale \\
Uso comune & Linguistica teorica & Parsing computazionale \\
Efficienza & Più complessa & Più semplice \\
\hline
\end{tabular}
\end{center}

\medskip
\noindent
In sintesi:
\begin{itemize}
    \item La sintassi a \textbf{costituenti} mostra \textit{come le parole si combinano} in gruppi gerarchici.
    \item La sintassi a \textbf{dipendenze} mostra \textit{chi dipende da chi}, evidenziando le relazioni grammaticali tra parole.
\end{itemize}

\qs{}{Che cos'è la misura PARSEVAL?}

\paragraph{Risposta:} si tratta di uno standard di valutazione per parser sintattici per confrontare alberi di costituenza generati automaticamente con alberi di riferimento (gold).

\qs{}{Che cos'è un treebank?}

\paragraph{Risposta:}
Un \textbf{treebank} è un corpus linguistico annotato che contiene un insieme di frasi accompagnate dalla loro \textbf{struttura sintattica} (e, in alcuni casi, anche semantica).

\medskip
Ogni frase nel treebank è arricchita con una rappresentazione strutturale sotto forma di:
\begin{itemize}
    \item \textbf{alberi a costituenti} (phrase structure trees), che mostrano come le parole si raggruppano in sintagmi;
    \item oppure \textbf{alberi a dipendenze} (dependency trees), che indicano le relazioni grammaticali tra le parole.
\end{itemize}

\medskip
I treebank sono utilizzati per:
\begin{itemize}
    \item addestrare e valutare parser sintattici automatici,
    \item studiare fenomeni grammaticali nelle lingue naturali,
    \item costruire risorse linguistiche per il \textit{Natural Language Processing} (NLP).
\end{itemize}

\medskip
\noindent
\textbf{Esempi di treebank famosi:}
\begin{itemize}
    \item \textbf{Penn Treebank} (inglese, sintassi a costituenti)
    \item \textbf{Universal Dependencies} (molte lingue, sintassi a dipendenze)
    \item \textbf{TUT} (Turin University Treebank, italiano)
\end{itemize}

\qs{}{Quali sono le fasi della NLG simbolica?}

\paragraph{Risposta:} La NLG simbolica tradizionale segue una pipeline modulare composta da sei fasi fondamentali, secondo il modello \fancyglitter{Reiter \& Dale} (2000):

\begin{enumerate}
\item \fancyglitter{Content Determination}
\begin{itemize}
    \item \textbf{Scopo}: Selezionare le informazioni da verbalizzare
    \item \textbf{Input}: Knowledge base o dati strutturati
    \item \textbf{Output}: Insieme di proposizioni logiche
    \item \textbf{Esempio}: Da DB meteo a $\{\text{temp}(oggi, 22°C), \text{pioggia}(domani, 80\%)\}$
\end{itemize}

\item \fancyglitter{Document Planning}
\begin{itemize}
    \item \textbf{Scopo}: Organizzare la struttura del discorso
    \item \textbf{Sottofasi}:
    \begin{itemize}
        \item \textit{Macroplanning}: Ordinamento delle informazioni
        \item \textit{Microplanning}: Raggruppamento concettuale
    \end{itemize}
    \item \textbf{Tecniche}: Rhetorical Structure Theory (RST)
\end{itemize}

\item \fancyglitter{Lexicalization}
\begin{itemize}
    \item \textbf{Scopo}: Mappare concetti a parole
    \item \textbf{Decisioni}:
    \begin{itemize}
        \item Scelta lessicale (\textit{"acquistare"} vs \textit{"comprare"})
        \item Classi di parole (verbo vs nominalizzazione)
    \end{itemize}
    \item \textbf{Risorse}: Ontologie lessicali (WordNet)
\end{itemize}

\item \fancyglitter{Referring Expression Generation}
\begin{itemize}
    \item \textbf{Scopo}: Introdurre entità nel discorso
    \item \textbf{Strategie}:
    \begin{itemize}
        \item Forme definite (\textit{"il libro"})
        \item Descrizioni (\textit{"il romanzo pubblicato nel 2020"})
        \item Pronominalizzazione
    \end{itemize}
    \item \textbf{Algoritmi}: Full Brevity, Incremental
\end{itemize}

\item \fancyglitter{Aggregation}
\begin{itemize}
    \item \textbf{Scopo}: Combinare proposizioni
    \item \textbf{Operazioni}:
    \begin{itemize}
        \item Coordinazione (\textit{"e"})
        \item Subordinazione (\textit{"perché"})
        \item Ellissi
    \end{itemize}
    \item \textbf{Esempio}: \textit{"La temperatura è 22°C"} + \textit{"Il cielo è sereno"} → \textit{"La temperatura è 22°C con cielo sereno"}
\end{itemize}

\item \fancyglitter{Linguistic Realization}
\begin{itemize}
    \item \textbf{Scopo}: Generare la superficie linguistica
    \item \textbf{Compiti}:
    \begin{itemize}
        \item Flessione morfologica
        \item Ordinamento parole
        \item Inserimento articoli/preposizioni
    \end{itemize}
    \item \textbf{Strumenti}: Realizer come SimpleNLG
\end{itemize}
\end{enumerate}
\qs{}{HMM è un modello generativo o discriminativo?}

\paragraph{Risposta:} HMM è un modello generativo: per capire se in una foto c'è un cane si guardano le caratteristiche che lo rendono un cane (e.g. la coda, etc.). MEMM invece è discriminativo: se si vede un guinzaglio non è un cane. 

\qs{}{Spiegare cosa si intende per anatomia di un parser.}

\paragraph{Risposta:}

\begin{itemize}
  \item Grammatica o modello di automa. 
  \item Algoritmo:
    \begin{itemize}
      \item Strategia di ricerca. 
      \item Organizzazione della memoria.
    \end{itemize}
  \item Oracolo.
\end{itemize}

\qs{}{Quali sono i 6 fenomeni del dialogo tra umani?}

\paragraph{Risposta:}

\begin{itemize}
  \item \fancyglitter{Turni:} 
    \begin{itemize}
      \item Ogni contributo alla conversazione. 
      \item È come se la comunicazione venisse vista come un gioco.  
      \item Il problema del turno: Come si capisce quando prendere il turno? Si può concedere all'utente di interrompere il sistema. La difficoltà sta nella comprensione del sistema, poiché può capitare che le persone smettano di parlare temporanemente nel proprio turno.
    \end{itemize}
  \item \fancyglitter{Speech Acts:}
    \begin{itemize}
      \item Constativi: affermare qualcosa.
      \item Direttivi: tentativo del parlante di far fare qualcosa all'altra persona.
      \item Committivi: riguardanti azioni future del parlante.
      \item Riconoscitivi: esprimere un'attitudine riguardo la persona che sta ascoltando.
    \end{itemize}
  \item \fancyglitter{Grounding:}
    \begin{itemize}
      \item I partecipanti alla conversazione hanno bisogno di un "terreno comune". 
      \item Consiste nel confermare che l'ascoltatore abbia capito. 
      \item Esempio: il bottone di un ascensore che si illumina\footnote{Tranne in dipartimento I guess.}.
    \end{itemize}
  \item \fancyglitter{Struttura del Dialogo:}
    \begin{itemize}
      \item \evidence{Punti di Adiacenza:} si ha una struttura sensata. 
      \item Domanda e Risposta. 
      \item Proposta e Accettazione/Rifiuto.
    \item \evidence{Sottodialoghi:} il dialogo può specializzarsi in un sottodialogo collegato e poi ritonare al punto da cui si è partiti (struttura ricorsiva). 
    \item \evidence{Presequenza:} chiedere un qualcosa e successivamente confermarlo.
    \end{itemize}
  \item \fancyglitter{Iniziativa:}
    \begin{itemize}
      \item Alcune conversazioni sono controllate da una sola persona. 
      \item Ma diverse conversazioni umane hanno un'iniziativa mista. 
      \item Tuttavia questo è difficile da replicare per cui solitamente si ricorre o all'iniziativa dell'utente (da cui derivano i vari ChatBOTs) o all'iniziativa del sistema (il sistema guida la conversazione su binari prestabiliti). 
    \end{itemize}
  \item \fancyglitter{Inferenza:}
    \begin{itemize}
      \item Da ciò che viene detto si deve ricavare il significato. 
      \item I principi di inferenza sulle conversazioni di Grice (principio di massima rilevanza, etc.).
    \end{itemize}
\end{itemize}


\qs{}{Cos'è il PoS Tagging?}

\paragraph{Risposta:} il Part of Speech tagging è il task in cui si fornisce in input una frase e viene restituita in output una frase taggata con le varie parti della frase (NOUN, VERB, ADJ, etc.). Molte parole sono ambigue (hanno più PoS associati), quindi è necessario disambiguare con un algoritmo. Il 60\% dei token sono ambigui. 

\paragraph{È importante perché:}

\begin{itemize}
  \item La pronuncia di parole dipende dal PoS. 
  \item Serve per analisi di Noun Phrases (shallow parsing). 
  \item Serve per l'analisi sintattica. 
  \item Per Machine Translation (con lingue simili). 
  \item Per sentiment analysis. 
  \item Da un punto di vista linguistico: capire le parti più utilizzate e quando.
\end{itemize}

\qs{}{Cos'è il NER Tagging?}

\paragraph{Risposta:} il Named Entities recognition è un task di tagging che si applica unicamente ai nomi propri. Per distinguere se indicano persone, organizzazioni, luoghi, etc. A differenza del PoS tagging è un multi word: New York City, Marie Curie, etc. Si tratta di semantica lessicale (sapere che Napoli è un luogo condiziona l'interpretazione semantica rispetto al sapere che Napoli è una squadra). 

\paragraph{2 Sottotask:}

\begin{itemize}
  \item Trovare lo span di testo che costituisce i nomi propri. 
  \item Taggare il tipo di entità.
\end{itemize}

\paragraph{Perché si vuole fare NER?}

\begin{itemize}
  \item Sentiment analysis: i sentimenti di un consumatore verso un prodotto. 
  \item Q\&A: rispondere a domande su entità. 
  \item Information extraction: estrarre fatti su entità da un testo.
\end{itemize}

\paragraph{Come si fa?}

\begin{itemize}
  \item BIO Tagging: Begin Inside Outside.
  \item L'idea è che ci si segna l'inizio (begin), quando si è dentro (inside) e quando si esce (outside).
  \item I tag necessari sono $2n + 1$ dove $n$ è il numero di diverse NE possibili.
\end{itemize}

