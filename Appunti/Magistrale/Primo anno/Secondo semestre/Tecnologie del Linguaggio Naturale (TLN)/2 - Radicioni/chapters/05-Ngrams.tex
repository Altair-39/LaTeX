\chapter{Rappresentazioni Probabilistiche e Vettoriali di Significato}

\section{Modelli Probabilistici}

\qs{}{Perché assegnare una probabilità a una frase:}

\begin{itemize}
  \item Traduzione. 
  \item Correzione di spelling. 
  \item Speech recognition. 
  \item Summarization. 
  \item Q\&A.
\end{itemize}

\dfn{Modello Probabilistico del Linguaggio (LM)}{
  Un modello che assegna probabilità a una sequenza di parole. 
}

\subsection{Ngrams}

\paragraph{Il modello più semplice è \fancyglitter{n-grams}, ossia sequenze di $n$ word:}

\begin{itemize}
  \item 2-gram (bigramma): sequenza di due parole. 
  \item 3-gram (trigramma): sequenza di tre parole.
\end{itemize}

\nt{Gli n-grams possono essere usati per stimare la probabilità dell'ultima parola date le parole precedenti assegnando una probabilità all'intera sequenza.}

\paragraph{Questa probabilità può essere stimata contando:}

\begin{itemize}
  \item Dato un corpus molto grande contare il numero di volte in cui si vede una determinata parola. 
  \item Predire una parola $w$ data una storia $h$ equivale a calcolare $P(w|h)$. 
  \item Rappresentiamo la probabilità di una variabile casuale $X_i$ assumendone il valore. E.g. "the" rappresentato come $P($the$)$ e non come $P(X_i = $the$)$.
  \item Una sequenza di N parole può essere rappresentata come $w_1\dots w_n$ e la relativa probabilità come $P(w_1\dots w_n)$.
\end{itemize}
\qs{}{Come calcolare le probabilità per un'intera sequenza?}

$$
P(X_1, \ldots, X_n) = P(X_1)P(X_2 \mid X_1)P(X_3 \mid X_{1:2}) \cdots P(X_n \mid X_{1:n-1}) = \prod_{k=1}^{n} P(X_k \mid X_{1:k-1})
$$

\nt{Tuttavia il linguaggio è creativo e quindi una particolare contesto potrebbe non essersi mai presentato.}

\paragraph{Intuizione:} 

\begin{itemize}
  \item La storia può essere approssimata considerando solo le ultime poche parole (assunzione di Markov). 
  \item $P(w_n \mid w_{1:n-1}) \approx P(w_n \mid w_{n-1})$
\end{itemize}


\dfn{Bigramma}{
  In un bigramma la storia è ristretta solo alla parola precedente. 
$$P(w_1 w_2\mid w_i) = P(w_1) \times P(w_2|w_1) \times \dots \times P(w_i|w_{i-1})$$

La probabilità condizionale è: $P(w_i|w_{i-1}) = \frac{|w_{i-1}*w_i|}{|w_{i-1}|}$.

}

\paragraph{Generazione di sequenze:}

\begin{itemize}
  \item Solo specifiche sequenze possono essere costruite con un modello di linguaggio senza ridurre le probabilità a 0. 
  \item Sequenze che compaiono nel training set. 
  \item Si possono generare sequenze di lunghezza arbitraria ma non c'è garanzia che saranno grammaticalmente corrette.
\end{itemize}

\dfn{Maximum Likehood Estimation}{
  Si ottiene un conteggio da un corpus e lo si normalizza tra 0 e 1. Le frequenze relative sono un mezzo per stimare le probabilità.
}

\paragraph{Conoscenza Sintattica:}

\begin{itemize}
  \item Le probabilità  raccolgono le regolarità sintattiche. Esempio: "to" ha un'alta probabilità di essere seguito da un verbo. 
\end{itemize}

\nt{Dai bigrammi si può passare a trigrammi, tetragrammi, etc.}

\paragraph{Log probabilities:}

\begin{itemize}
  \item Le probabilità sono minori o uguali a 1 quindi più probabilità moltiplichiamo assieme più il prodotto diventa piccolo (rischio di underflow). 
  \item Per cui vengono impiegate le probabilità logaritmiche (sommare è più veloce che moltiplicare): $p_1 \times p_2 = log(p\_1) + log(p_2)$.
\end{itemize}

\subsection{Valutare un LM}

\paragraph{Tipi di valutazione:}

\begin{itemize}
  \item \fancyglitter{Valutazione Estrinseca:} usare gli LMs in applicazioni e vedere come cambiano le performance. 
  \item \fancyglitter{Valutazione Intrinseca:} misurare la qualità di un modello indipendentemente da una data applicazione: 
    \begin{itemize}
      \item Le probabilità di un ngram sono acquisite da un training set e la sua qualità può essere testata su un test set. 
      \item Questo tipo di valutazione implica il partizionamento dei dati in due sets distinti e comparare quanto il modello allenato si adatti al test set.
    \end{itemize}
\end{itemize}

\dfn{Perplessità}{
  La perplessità (perplexity) misura quanto bene un modello linguistico predice una sequenza di parole non vista. 
  
  Consideriamo una sequenza di parole di lunghezza $k$: 
  \[
  W = \{w_1, w_2, \dots, w_k\}
  \]
  
  Dato un modello linguistico $\text{LM}$, la probabilità della sequenza $W$ è:
  \[
  \text{LM}(W) = \prod_{i=1}^{k} \text{LM}(w_i \mid w_{1:i-1})
  \]

  La log-probabilità media è:
  \[
  \frac{1}{k} \sum_{i=1}^{k} \log \text{LM}(w_i \mid w_{1:i-1})
  \]

  La perplessità della sequenza $W$ è quindi definita come:
  \[
  \text{PPL}(\text{LM}, W) = \exp\left\{ -\frac{1}{k} \sum_{i=1}^{k} \log \text{LM}(w_i \mid w_{1:i-1}) \right\}
  \]

}

\nt{Valori di PPL più bassi indicano che il modello è più abile nel predire la sequenza, ovvero assegna probabilità più alte alle parole corrette.}

\subsection{Smoothing}

\qs{}{Cosa succede se delle parole sono nel nostro vocabolario, ma appaiono in un test set in un nuovo contesto?}

\nt{Si vuole impedire che quelle parole abbiano zero probabilità.}

\dfn{Laplace Smoothing}{
  La \textbf{Laplace smoothing} è una tecnica di smoothing che consiste nell’aggiungere uno a ciascun conteggio prima della normalizzazione in probabilità.

  Per i \textbf{unigrammi}, la stima di massima verosimiglianza per la probabilità della parola $w_i$ è:
  \[
  P(w_i) = \frac{c_i}{N}
  \]
  dove $c_i$ è il conteggio della parola $w_i$ e $N$ è il numero totale di token.

  Con la Laplace smoothing, si aggiunge 1 a ogni conteggio, e si regola il denominatore per tener conto delle $V$ parole nel vocabolario:
  \[
  P_{\text{Lap}}(w_i) = \frac{c_i + 1}{N + V}
  \]

  In alternativa, si può descrivere l'effetto dello smoothing attraverso un \textit{conteggio corretto}:
  \[
  c_i^* = (c_i + 1) \cdot \frac{N}{N + V}
  \]

  Questo conteggio corretto viene poi normalizzato su $N$ per ottenere una nuova probabilità:
  \[
  P_i^* = \frac{c_i^*}{N}
  \]

  Lo smoothing può anche essere visto come una forma di \textbf{discounting}, cioè una riduzione di alcuni conteggi per redistribuire la massa di probabilità anche su eventi precedentemente con probabilità nulla. Il \textit{discount relativo} $d_c$ è:
  \[
  d_c = \frac{c^*}{c}
  \]
}

\cor{Normalizzazione}{
  Le probabilità dei bigrammi si ottengono normalizzando ciascun conteggio di bigramma rispetto al conteggio del primo elemento del bigramma:
  \[
  P(w_n \mid w_{n-1}) = \frac{C(w_{n-1} w_n)}{C(w_{n-1})}
  \]

  Con la \textbf{Laplace smoothing} (add-one), si aggiunge 1 al numeratore e si incrementa il denominatore con $V$, il numero di parole distinte nel vocabolario:
  \[
  P_{\text{Lap}}(w_n \mid w_{n-1}) = \frac{C(w_{n-1} w_n) + 1}{C(w_{n-1}) + V}
  \]

  Questo garantisce che nessun bigramma abbia probabilità nulla, anche se non è stato osservato nel corpus di addestramento.

  \textit{Esempio:} se $V = 1446$, ogni conteggio unigramma nel denominatore sarà aumentato di 1446.
}


\section{Modello di Spazio Vettoriale}


