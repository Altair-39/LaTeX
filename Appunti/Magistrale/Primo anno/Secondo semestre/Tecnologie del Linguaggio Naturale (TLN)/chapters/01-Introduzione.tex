\chapter{Introduzione alle Tecnologie del Linguaggio Naturale}

\section{Prologo}

La prima parte del corso sarà incentrata sulla linguistica computazionale generale, in cui ci si soffermerà sugli aspetti più tradizionali e linguistici\footnote{Libro di riferimento: An Introduction to Natural Language Processing,
Computational Linguistics, and Speech Recognition. La prima e la seconda edizione, perché Jurafsky non riesce a finire il draft della terza :(}. In questa parte verrà anche trattato il parsing. Nella seconda parte si andranno a studiare la semantica lessicale e le ontologie. Infine, nella terza parte del corso si andrà a studiare NLP statistico e distribuzionale.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.75]{01/key.png}
    \caption{Il giorno prima dell'esame bisogna sapere cosa significano tutte queste parole :3}
\end{figure}

\paragraph{Le 4 ere della linguistica computazionale:}

\begin{enumerate}
  \item 1940 - 1969: primi tentativi. 
  \item 1970 - 1992: formalizzazione. 
  \item 1993 - 2012: apprendimento automatico. 
  \item 2013 - 2018: deep learning.
\end{enumerate}

\nt{Tutto cambiò nel 2018, quando NLP fu il primo successo su larga scala di rete neurale autosupervisionata.}

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.75]{01/timeline.png}
    \caption{Il passato delle tecnologie del linguaggio naturale.}
\end{figure}

\subsection{La Complessità del Linguaggio Naturale}

C'è un legame tra linguaggio umano e intelligenza. Già Turing sosteneva che se si potesse parlare in un certo modo si fosse intelligenti (test di Turing). La differenza tra il linguaggio umano e un linguaggio di programmazione è l'\fancyglitter{ambiguità}: C o Java non sono ambigui.

\paragraph{Il linguaggio umano:}

\begin{itemize}
  \item \fancyglitter{Discretezza} (esistenza di elementi): 
    \begin{itemize}
      \item Api: Ritmo, orientamento, durata. 
      \item Esseri umani: Fonemi, morfemi, parole.
    \end{itemize}
  \item \fancyglitter{Ricorsività}: 
    \begin{itemize}
      \item Scimpanze: Gesti atomici. 
      \item Uomo: Gianni vede Pietro, Maria vuole che Gianni veda Pietro, Paolo crede che Maria voglia che Gianni veda Pietro.
    \end{itemize}
  \item \fancyglitter{Dipendenza dalla struttura}: 
    \begin{itemize}
      \item Non “una parola dietro l'altra” ma c'è una struttura: La ragazza parte, I ragazzi di cui mi ha parlato la ragazza partono. 
    \end{itemize}
  \item \fancyglitter{Località}: 
    \begin{itemize}
      \item Gianni lo ha guardato. 
      \item Gianni ha detto che Pietro lo ha guardato.
    \end{itemize}
\end{itemize}

\paragraph{Intelligenza e linguaggio nel il test di Turing:}

\begin{itemize}
  \item Possono le macchine pensare?
  \item Se riesco a parlare come un essere umano allora penso. 
  \item Gioco dell'imitazione: un giudice deve capire se quello che ha davanti è un uomo oppure un computer.
\end{itemize}

\nt{Ci sono una serie di obiezioni a questo test: teologia, matematica, coscienza, etc.}

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.6]{01/Turing.png}
    \caption{Il gioco dell'imitazione.}
\end{figure}

\subsubsection{}

Nel 1966, Weizenbaum crea Eliza. Una macchina in grado di "comprendere" e ingannare gli esseri umani.

\nt{Il punto debole del test di Turing e di Eliza è il giudice: se è coinvolto emotivamente potrebbe far passare un computer per un essere umano\footnote{Blade runner moment}.}

\dfn{Winograd Schema}{
  Evoluzione del Turing test: un test a scelta multipla che utilizza domande con una specifica struttura. In questi test gli esseri umani sono molto bravi a rispondere, i computer no. 
}

\nt{Rimuove il giudizio, quindi tecnicamente più accurato.}

\cor{Captcha}{
  Un test di Turing inverso per capire se l'interloquitore è umano. Non c'è linguaggio, ma riconoscimento cognitivo.
}

\cor{Voight-Kampff Test}{
  Test in Blade runner basato sulle emozioni, evoluzione del test di Turing.
}

\subsection{I Livelli di Conoscenza del Linguaggio}

HAL 9000, in "2001: Odissea nello spazio" mostra un esempio di comunicazione. 

\qs{}{Come fa HAL a rispondere?}

\begin{itemize}
  \item Riconoscimento vocale. 
  \item Comprensione del linguaggio naturale. 
  \item Generazione del linguaggio naturale. 
  \item Sintesi vocale. 
  \item Recupero ed estrazione di informazioni. 
  \item Inferenza.
\end{itemize}

\paragraph{Livelli della conoscenza:}

\begin{enumerate}
  \item Il suono: HAL deve essere in grado di analizzare e produrre
dei segnali audio che contengono le parole: foni e
fonemi. 
\item Le parole: HAL deve essere in grado di riconoscere le singole
parole. 
\item Raggruppare le parole: HAL deve essere in grado di distinguere la struttura
della frase. 
\item Significato: HAL deve conoscere il significato delle singole
parole e deve essere in grado di comporre questi
significati per trovare il significato complessivo
della frase. 
\item Contesto e scopi: HAL deve avere delle conoscenze del mondo che
gli permettono usare il linguaggio in maniera
contestuale: \textit{I’m afraid, I can’t} invece di \textit{I won't}.
\item Conversazione: HAL deve avere deve essere in grado di
conversare, dando delle risposte e facendo delle
domande pertinenti al discorso. 
\end{enumerate}

\paragraph{A ogni livello corrisponde una parte del linguaggio:}

\begin{enumerate}
  \item Fonetica e Fonologia: lo studio del suono della lingua. 
  \item Morfologia: lo studio delle parti significative delle parole. 
  \item Sintassi: lo studio sulla struttura e sulle relazioni tra le parole. 
  \item Semantica: lo studio del significato. 
  \item Pragmatica: lo studio di come il linguaggio è usato per compiere goal. Il passivo serve per mettere in luce/enfatizzare alcune parti della frase. 
  \item Discorso: lo studio delle unità linguistiche rispetto alla singola dichiarazione.
\end{enumerate}

\nt{Jurafsky è un chad nerd.}

\subsection{Strutture Linguistiche e Ambiguità}

Analizzando i vari livelli si trovano diverse \fancyglitter{strutture linguistiche}. 

\dfn{Struttura Linguistica}{
Una struttura è un insieme su cui è definita una
relazione: 
\begin{itemize}
  \item Relazione fonetico-fonologica sull'insieme dei foni-fonemi. 
  \item Relazione morfologica sull'insieme dei morfemi.
  \item Relazione sintattica sull'insieme delle parole. 
  \item Relazione semantica sull'insieme dei significati delle parole. 
  \item Relazione pragmatica sull'insieme dei significati delle parole e sul contesto. 
  \item Relazione “discorsale” sull'insieme delle frasi.
\end{itemize}
}

\nt{
    Ci sono relazioni tra i componenti della frase. Inoltre le relazioni cambiano a seconda della lingua.
}

\ex{Struttura Sintattica}{
  \begin{center}
    \includegraphics[scale=0.8]{01/sintassi.png}
  \end{center}
}

\dfn{Ambiguità}{
  Il linguaggio  naturale presenta frasi che possono essere interpretate in modi differenti.
}

\ex{Ambiguità}{
  \begin{center}
    "I made her duck"
  \end{center}

  \begin{itemize}
    \item Ho cucinato una papera per lei. 
    \item Ho cucinato una papera che apparteneva a lei. 
    \item Ho creato una papera con la stampante 3D e gliel'ho data a lei.
    \item Ho fatto abbassare la sua testa.
    \item In Harry Potter\footnote{Rowling merda.}: Ho trasformato lei in una papera.
  \end{itemize}
}

\clm{}{}{
  \begin{itemize}
    \item Le parole "duck" e "her" sono morfologicamente ambigue nella loro parte del discorso. "Duck" può essere un verbo o un nome, "her" può essere un pronome dativo o possessivo. 
    \item Il verbo "make" è sintetticamente ambiguo: può essere transitivo o intransitivo. 
    \item Inoltre "make" è anche semanticamente ambiguo: può significare creare o cucinare. 
    \item In una frase parlata c'è un altro livello per cui "her" può essere udito come "eye" e "make" come "maid".
  \end{itemize}
}

\nt{Essere ambigui permette di essere brevi e coincisi.}

\paragraph{Altre proprietà notevoli del linguaggio:}

\begin{itemize}
  \item Linguaggio non standard, evolve nel tempo. 
    \begin{itemize}
      \item Scialla bros $\rightarrow$ chill $\rightarrow$ è easy. 
      \begin{center}
    \includegraphics[scale=0.5]{01/burns.png}
  \end{center}
    \end{itemize}
  \item Segmentazione. 
    \begin{itemize}
      \item Il treno Torino San Remo. 
    \end{itemize}
  \item Locuzioni, spesso l'interpretazione non è composizionale. 
    \begin{itemize}
      \item Pollica verde.
    \end{itemize}
  \item Neologismi. 
    \begin{itemize}
      \item Twettare\footnote{Musk merda.}
    \end{itemize}
  \item Conoscenza del mondo. 
    \begin{itemize}
      \item Lucia e Carola erano sorelle. 
      \item Lucia e Carola erano madri.
    \end{itemize}
  \item Meta-linguaggio. 
    \begin{itemize}
      \item La prima cosa bella ha avuto un
grandissimo successo.
    \end{itemize}
\end{itemize} 

\subsection{Lo Stato dell'Arte}

\begin{itemize}
  \item 1976: In Canada un sistema riesce a stampare due bollettini meteo in due lingue diverse. \item BabelFish, di Yahoo, era un sistema "a regole" di trascrizione automatica, basato su Systran.
  \item 2011: IBM costruisce un supercomputer per battere un essere umano a Jeopardy, Watson.  
  \item Tecnologie vocali: Speech Recognition, TextToSpeech, HTML5 Speech API (pagine web vocali).
\end{itemize}

\nt{
  Dopo sette milioni e mezzo di anni Pensiero Profondo fornisce la
  risposta: "42"\footnote{Guida Galattica per gli Autostoppisti.}. 
}

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.75]{01/timeline2.png}
    \caption{LLM. Tratto da "Hands-On Large Language Models", uscito nel Dicembre del 2024.}
\end{figure}

\nt{Well, Deepseek è open source e funziona meglio di ChatGPT (a patto che non chiedi cosa sia successo a piazza Tienanment nel 1989).}

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.6]{01/data.png}
    \caption{Shifting di paradigma dovuto al Machine Learning.}
\end{figure}

\dfn{AI Generativa}{
  Modello di linguaggio di reti neurali multi-task basate sui transformer addestrati su una grande quantità di dati utilizzando self training e feedback umano.
}

\begin{itemize}
  \item Modello di Linguaggio: Text prediction $\rightarrow$ T9. 
  \item Multi-task: Google Translator, Siri.
\end{itemize}

\qs{}{Come fare un LLM (M. Lapata)?}

\begin{enumerate}
  \item Collezionare una grande quantità di dati. 
  \item Chiedere al LLM di predirre la nuova parola in una frase. 
  \item Ripetere il tutto.
\end{enumerate}

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.8]{01/addestramento.png}
    \caption{Auto addestramento di una rete neurale.}
\end{figure}

\qs{}{Come usare un LLM?}

\begin{itemize}
  \item Sintonizzazione a grana fine.
  \item Prompting.
\end{itemize}

\paragraph{Si può usare un LLM per:}

\begin{itemize}
  \item Search Engine. 
  \item Writer/Code assistant.
\end{itemize}

\nt{Noam Chomsky odia questi sistemi. Secondo lui servono per evitare l'apprendimento.}

\paragraph{DeepSeek:}

\begin{itemize}
  \item Apprendimento rinforzato automatico (senza essere umani). 
  \item Meno costoso $\rightarrow$ politicamente importante.
\end{itemize}

\paragraph{\fancyglitter{Il problema fondamentale}:} Convertire una frase o un testo in una forma che
permetta l'applicazione di meccanismi di
ragionamento automatico.


