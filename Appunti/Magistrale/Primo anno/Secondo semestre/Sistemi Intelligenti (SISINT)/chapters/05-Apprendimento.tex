\chapter{Apprendimento e Reti Neurali}

\section{Apprendimento}

\subsection{Introduzione alla Classificazione}

\paragraph{Il problema:}

\begin{itemize}
	\item Dati:
	      \begin{itemize}
		      \item Esempi.
		      \item Categorie/classi.
	      \end{itemize}
	\item Costruire:
	      \begin{itemize}
		      \item Una rappresentazione astratta (modello) che permetta di
		            associare in modo corretto nuove istanze alla classe (o alle
		            classi) di appartenenza.
	      \end{itemize}
\end{itemize}

\dfn{Apprendimento Supervisionato}{
	Gli esempi dal quale astrarre le definizioni delle classi hanno associata la classe a cui appartengono.
}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.4]{05/apprendimento.png}
	\caption{Schema generale.}
\end{figure}

\cor{Learning Set}{
	Per learning (o training) set si intende la collezione di dati usati per svolgere il
	compito di apprendimento. I dati sono divisi in istanze (o record o esempi). Ogni
	esempio è rappresentato da una tupla (x, y) dove x è a sua volta una tupla di valori
	di attributi descrittivi e y è la classe di appartenenza dell'istanza.
}

\paragraph{Uso dei modelli appresi:}

\begin{itemize}
	\item \fancyglitter{Predittivo:} viene usato per predire la classe
	      di appartenenza di istanze ignote
	      in fase di apprendimento.
	\item \fancyglitter{Descrittivo:} Viene usato come strumento esplicativo
	      che permette di evidenziare quali carat-
	      teristiche distinguono le diverse categorie.
\end{itemize}

\qs{}{Ma qual è la bontà dei modelli appresi?}

\dfn{Valutazione Sperimentale}{
	Il modello viene usato per classificare le istanze di un
	test set. La valutazione della bontà è fatta sulla base del comportamento di classificazione corretto/sbagliato su questi dati.
}

\paragraph{Proprietà:}

\begin{itemize}
	\item \fancyglitter{Accuratezza} = predizioni corrette / predizioni totali.
	\item \fancyglitter{Error Rate} = predizioni sbagliate / predizioni totali.
\end{itemize}

\cor{Matrice di Confusione}{
	Matrice quadrata $N x N$ (con $N$ numero di classi). Le righe indicano le classi reali di appartenenza, le colonne indicano le classi predette.
}

\nt{L'ideale è che tutte le predizioni stiano sulla diagonale principale.}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.35]{05/confusion.png}
	\caption{Matrice di confusione.}
\end{figure}

\cor{Matrice dei Costi}{
	Matrice $N x N$ (con $N$ numero di classi). Associa un costo allo indovinare/sbagliare una predizione.
}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.35]{05/cost.png}
	\caption{Matrice dei costi.}
\end{figure}

\subsection{Costruire un Modello}

\dfn{Rote Learning (Apprendimento Meccanico)}{
	Si tratta di memorizzare le varie istanze. Tramite confronti cerca un’istanza
	identica:
	\begin{itemize}
		\item Se la trova restituisce la classe corrispondente.
		\item Se non la trova prova a indovinare (cerca istanze simili utilizzando una misura di distanza).
	\end{itemize}
}

\paragraph{Strategie per decidere:}

\begin{itemize}
	\item Votazione a Maggioranza: la classe più votata vince.
	\item Votazione pesata: ogni voto ha un peso maggiore/minore a seconda
	      della “distanza” fra le istanze considerate.
\end{itemize}

\nt{
	In caso di votazione pesata, i pesi vengono
	calcolati, usati e poi dimenticati.
}

\paragraph{Algoritmi di apprendimento diversi producono modelli di tipo diverso:}

\begin{itemize}
	\item Alberi di decisione: albero.
	\item Sistemi a regole: if-then.
	\item Reti neurali: matrici di numeri.
	\item Apprendimento per rinforzo: distribuzioni di
	      probabilità e matrici di numeri.
\end{itemize}

\nt{Nell'apprendimento automatico non sono importanti i numeri, ma cosa essi rappresentato e come sono ottenuti.}

\dfn{Alberi di Decisione}{
	Sono strumenti di supporto alle decisioni che
	usano modelli strutturati ad albero, comunemente utilizzati per esempio per la definizione
	di strategie mirate al conseguimento di un goal.
}

\nt{Per esempio i sottomenu a tendina sono alberi di decisione.}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.3]{05/dec.png}
	\caption{Struttura di un albero di decisione.}
\end{figure}

\clm{}{}{
	\begin{itemize}
		\item Ogni test è su un attributo.
		\item Le foglie sono classi.
		\item A ogni branch dell'albero si prende una decisione sulla base di un test e si scende al nodo successivo.
		\item Un dataset noto è il dataset degli iris.
	\end{itemize}
}

\paragraph{Tipi di attributi:}

\begin{itemize}
	\item Binari: booleani.
	\item Nominali: che hanno un nome.
	\item Ordinali: per cui vale un ordine.
	\item Continui.
\end{itemize}

\dfn{Algoritmo di Hunt}{
	L'albero viene costruito procedendo ricorsivamente e suddividendo il learning
	set in sottoinsiemi via via più “puri”.

	Dati:

	\begin{itemize}
		\item $D_t$ = sottoinsieme del learning set associato al nodo $t$.
		\item $y = \{y_1, y_2, \dots, y_c\}$ = insieme delle etichette che identificano le classi.
	\end{itemize}
}

\paragraph{Algoritmo di Hunt:}

\begin{enumerate}
	\item Se tutte le istanze in $D_t$ appartengono alla stessa classe $y_t$ allora il nodo
	      è una foglia etichettata dalla classe $y_t$ delle sue istanze.
	\item Si sceglie un attributo fra quelli che descrivono le istanze, si produce un
	      nodo figlio per ogni possibile valore dell'attributo.
\end{enumerate}

\clm{}{}{
	\begin{itemize}
		\item Se una certa combinazione di valori non è rappresentata da nessuna
		      istanza, questa sarà associata alla classe di default (se esiste).
		\item Se tutte le istanze associate a un nodo sono identiche come tuple ma
		      corrispondono a classi differenti (non-determinismo), il nodo non può
		      essere scisso.
		\item Quando si termina la costruzione dell'albero?
		\item Come si sceglie l'attributo di split?
	\end{itemize}
}

\paragraph{Tipi di split:}

\begin{itemize}
	\item Su attributi binari: Il nodo corrente avrà due figli a seconda del
	      valore rappresentato. Gli esempi associati
	      al nodo radice verranno suddivisi fra i due
	      figli a seconda del valore riportato in corrispondenza dell'attributo.
	\item Su attributi multivalore: l nodo avrà tanti figli quanti sono i possibili valori dell'attributo.
	\item Su attributi nominali: l'attributo assume valori su un insieme (finito) di etichette $\{L_1, L_2, \dots, L_n\}$. Gli split possono essere binari oppure multivalore.
	\item Su attributi nominali: si possono avere split binari o multivalore con un vincolo, il
	      raggruppamento dei valori deve rispettare l'ordinamento.
	\item Binari di attributi continui: in questo caso il test prevedono l'identificazione di un valore possibile v per
	      l'attributo A in questione.
	\item Multivalore di attributi continui: in questo caso il test prevedono l'identificazione di un insieme di valori $v_i$ per
	      l'attributo A in questione e la produzione di una serie di test $v_i \leq A < v_{i+1}$
\end{itemize}

\paragraph{Bontà degli split:}

\begin{itemize}
	\item \fancyglitter{Criterio generale:} alberi compatti sono preferiti ad alberi che consentono di
	      raggiungere lo stesso grado di accuratezza (e di error rate) usando un maggior
	      numero di test. Sono preferiti gli split che producono nodi figli la cui estensione
	      prevede minore confusione (il cui grado di purezza è maggiore). Misure alternative: entropia, gini, errore di classificazione.
	\item \fancyglitter{Rasoio di Occam:} a parità di assunzioni, la spiegazione più semplice è da
	      preferire.
\end{itemize}

\dfn{Entropia}{
	Serve per capire quanto sia confuso un insieme, più bassa è meglio è.
	\[
		\text{Entriopia}(t) = - \displaystyle\sum_{i=0}^{c-1} p(i|t) \text{log}_2 p(i|t)
	\]
	Dove $P(i|t)$ è la probabilità che l'elemento appartenenga all' i-esima classe.

}

\cor{Calcolo del Guadagno}{
	Formula per calcolare il guadagno di uno split.
	\[
		\Delta = I(\text{parent}) - \displaystyle\sum_{j=1}^{k} \displaystyle\frac{N(v_j)}{N} I(v_j)
	\]
}

\paragraph{Guadagno:}

\begin{itemize}
	\item $\Delta$ = guadagno.
	\item $I(\text{parent})$ = impurità del nodo genitore.
	\item $N$ = numero record del nodo genitore.
	\item $N(v_j)$ = numero dei record del nodo figlio j-mo.
	\item $I(v_j)$ = impurità del figlio j-mo.
\end{itemize}

\nt{Si vuole minimizzare l'impurità e massimizzare il guadagno.}

\cor{Information Gain}{
	Per information gain si intende una misura del guadagno ottenuta usando
	l'entropia come valore dell'impurità dei nodi.
	\[
		\Delta = \text{entropia(parent)} - \displaystyle\sum_{j=1}^{k} \displaystyle\frac{N(v_j)}{N} \text{entropia}(v_j)
	\]
}

\nt{
	Le misure del grado di confusione, come Gini ed entropia tendono a favorire
	attributi che hanno molti valori diversi rispetto ad attributi con pochi valori alternativi.
}

\subsection{Overfitting}

Consideriamo la costruzione di un albero di decisione: ad ogni iterazione
occorre individuare un attributo su cui effettuare il test. Un attributo viene
preso in considerazione se il guadagno che dà supera una soglia minima.

\dfn{Overfitting}{
	Essere troppo adatto. Il modello che è stato costruito è troppo specializzato.
}

\clm{}{}{
	\begin{itemize}
		\item Il problema dell'overfitting nasce dal fatto che si utilizzano algoritmi \fancyglitter{greedy}.
		\item Questi algoritmi tendono a massimizzare il guadagno.
		\item Un modello addestrato in questo modo effettua una cattiva generalizzazione e non riesce a classificare nel mondo reale.
	\end{itemize}
}

\paragraph{Ridurre l'overfitting:}

\begin{itemize}
	\item Prepruning: si stabilisce una soglia per cui si smette la costruzione dell'albero. Ci saranno foglie con più classi diverse (si dovrà fare una scelta sulla classe da restituire).
	\item Postpruning: si applica al modello già addestrato. Vengono tagliati i percorsi meno usati o particolari.
\end{itemize}

\nt{Esistono altre tecniche come Minimum Description Length (MDL).}

\section{Reti Neurali}

\subsection{Ispirqaione Biologica}

Le reti neurali si ispirano al modo in cui i neuroni agiscono ed interagiscono.

\nt{I neuroni artificiali non sono modelli fedeli dei neuroni biologici, ne
	catturano solo alcuni principi.}

\dfn{Perceptron}{
	Un perceptron è un elemento computazionale, dotato di una piccola memoria
	in grado di calcolare una funzione di attivazione in esso strettamente codificata:
	\[
		Y = f(net)
	\]
	\[
		net = \displaystyle\sum_{i=1}^{n}w_i X_i
	\]
	Tale funzione è calcolata su una composizione dei valori in ingresso, opportuna-
	mente pesati.

}

\nt{Originariamente era usata la funzione gradino ($y = 0 | y = i$).}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.4]{05/per.png}
	\caption{Un percettrone.}
\end{figure}

\cor{Funzione Sigmoide}{
$Y = f(net) = \displaystyle\frac{1}{1+e^{-\alpha * (net - \theta)}}$

\begin{itemize}
	\item $\theta$ soglia o bias.
	\item $\alpha$ controlla la pendenza.
\end{itemize}
}

\begin{center}
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[scale=0.45]{05/sig.png}
	\end{minipage}%
	\hfill
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[scale=0.15]{05/sigma.png}
	\end{minipage}
\end{center}

\paragraph{Un percettrone codifica un test lineare:}

\begin{itemize}
	\item Ciò che cade al di sopra
	      dell'iperpiano codificato
	      dai suoi pesi fa attivare il
	      neurone.
	\item Ciò che è sotto
	      non fa attivare il neurone.
	\item Classificazione:
	      \begin{itemize}
		      \item Ciò che è sopra all'iperpiano
		            è riconosciuto come
		            appartenente alla classe
		            obiettivo.
		      \item Sotto all'iperpiano
		            le istanze negative.
	      \end{itemize}
\end{itemize}

\cor{Pesi}{I pesi sulle connessioni in ingresso definiscono la posizione e la pendenza
	dell'iperpiano nello spazio in cui sono definiti gli input. I pesi caratterizzano
	i neuroni e costituiscono la conoscenza del neurone.}

\paragraph{Caratteristiche del perceptron:}

\begin{itemize}
	\item Adatto a svolgere compiti di tipo numerico.
	\item Consente di risolvere problemi separabili linearmente.
	\item Conoscenza data dai pesi.
	\item I pesi sono persistenti.
	\item Apprendimento da esempi, supervisionato.
	\item Imparare = individuare la posizione corretta dell'iperpiano nello spazio.
\end{itemize}

\dfn{Epoca di Apprendimento}{
	Elaborare una volta tutte le istanze appartenenti al learning set.
}

\nt{Solitamente si utilizzano più epoche di apprendimento.}

\paragraph{Limiti del perceptron:}

\begin{itemize}
	\item Un perceptron non è in grado di capire un XOR.
	\item Interpreto le coppie come coordinate di punti e il risultato dello XOR come il fatto
	      che il punto debba stare sopra o sotto all'iperpiano.
	\item Un'iperpiano da solo non ce la fa (sono necessari più iperpiani).
\end{itemize}

\subsection{Nascita delle Reti Neurali}

\dfn{Rete Neurale}{
	Una rete neurale è un approssimatore universale di funzioni, avente natura
	distribuita. È costituita da un insieme di neuroni, collegati fra di loro secondo
	una topologia, che dipende dal modello di rete realizzato. I neuroni possono
	implementare funzioni diverse. Possono essere software oppure hardware.
}

\cor{Multi-Layer Perceptron}{
	Il multi-layer perceptron (MLP) è un modello di NN con topologia a strati,
	Feed-forward (flusso di calcolo in una sola direzione). Di solito i neuroni
	di input implementano la funzione identità. I neuroni hidden sono perceptron,
	che usano la sigmoide (o altre funzioni tipo a scalino, derivabili), i neuroni di
	output combinano i risultati dei neuroni hidden. Si possono avere più layer
	hidden.
}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.35]{05/mlp.png}
	\caption{Multi-Layer Perceptron.}
\end{figure}

